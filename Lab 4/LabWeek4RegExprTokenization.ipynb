{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "887071\n"
     ]
    }
   ],
   "source": [
    "# get the book Emma from the Gutenberg collection and keep as raw text\n",
    "file0 = nltk.corpus.gutenberg.fileids( ) [0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0)\n",
    "print(type(emmatext))\n",
    "print(len(emmatext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 20 characters in the str emmatext as one string\n",
    "print(emmatext[:50])\n",
    "emmatext[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "E\n",
      "m\n",
      "m\n",
      "a\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "J\n",
      "a\n",
      "n\n",
      "e\n",
      " \n",
      "A\n",
      "u\n",
      "s\n",
      "t\n",
      "e\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# print the first 20 characters in emmatext by iterating over the characters\n",
    "for c in emmatext[:20]:\n",
    "  print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monty PythonHoly Grail\n",
      "Monty Python and the Holy Grail\n"
     ]
    }
   ],
   "source": [
    "## Review of strings and string operation +\n",
    "string1 = 'Monty Python'\n",
    "string2 = 'Holy Grail'\n",
    "print(string1 + string2)\n",
    "print(string1 + ' and the ' + string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]  VOLUME I  CHAPTER I   Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace end-of-line character with a space\n",
    "# check table 3.2 in NLTK book for other string functions\n",
    "newemmatext = emmatext.replace('\\n', ' ')\n",
    "newemmatext[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Development of regular expressions for tokenizing text\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'book', 'is', 'interesting']\n"
     ]
    }
   ],
   "source": [
    "# pattern to match words, i.e. anything with a sequence of word characters, ignores special chars\n",
    "shorttext = 'That book is interesting.'\n",
    "pword = re.compile('\\w+')\n",
    "print(re.findall(pword, shorttext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster', 'print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "specialtext = 'That U.S.A. poster-print costs $12.40, but with 10% off.'\n",
    "print(re.findall(pword, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('That', ''), ('U', ''), ('S', ''), ('A', ''), ('poster-print', '-print'), ('costs', ''), ('12', ''), ('40', ''), ('but', ''), ('with', ''), ('10', ''), ('off', '')]\n",
      "[('end-of-line', '-line'), ('character', '')]\n"
     ]
    }
   ],
   "source": [
    "# pattern to match words with internal hyphens\n",
    "ptoken = re.compile('(\\w+(-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))\n",
    "print(re.findall(ptoken, 'end-of-line character'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n",
      "['end-of-line', 'character']\n"
     ]
    }
   ],
   "source": [
    "# ignore the group of the inner parentheses \n",
    "ptoken = re.compile('(\\w+(?:-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))\n",
    "print(re.findall(ptoken, 'end-of-line character'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U.S.A.']\n"
     ]
    }
   ],
   "source": [
    "# abbreviations like U.S.A.\n",
    "pabbrev = re.compile('((?:[A-Z]\\.)+)')\n",
    "print(re.findall(pabbrev, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "# combine this pattern with the words to make more general tokens\n",
    "ptoken = re.compile('(\\w+(?:-\\w+)*|(?:[A-Z]\\.)+)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "# switch the order of the patterns to first match abbreviations and then other words\n",
    "ptoken = re.compile('((?:[A-Z]\\.)+|\\w+(?:-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "# add expression for currency\n",
    "ptoken = re.compile('((?:[A-Z]\\.)+|\\w+(?:-\\w+)*|\\$?\\d+(?:\\.\\d+)?)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'That', ''), ('U.S.A.', '', ''), ('', 'poster-print', ''), ('', 'costs', ''), ('', '', '$12.40'), ('', 'but', ''), ('', 'with', ''), ('', '10', ''), ('', 'off', '')]\n"
     ]
    }
   ],
   "source": [
    "# this is an equivalent regular expression except that it has extra parentheses\n",
    "# the python string triple quote allows multi-line strings with end of line comments\n",
    "ptoken = re.compile(r'''((?:[A-Z]\\.)+) # abbreviations, e.g. U.S.A.\n",
    "   | (\\w+(?:-\\w+)*) # words with internal hyphens\n",
    "   | (\\$?\\d+(?:\\.\\d+)?) # currency, like $12.40\n",
    "   ''', re.X) # verbose flag\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### using NLTK's regular expression tokenizer\n",
    "# first define a multi-line string that is a regular expression\n",
    "pattern = r''' (?x) \t# set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+    # abbreviations, e.g. U.S.A.\n",
    "        | \\$?\\d+(?:\\.\\d+)?%?    # currency and percentages, $12.40, 50%\n",
    "        | \\w+(?:-\\w+)*  # words with internal hyphens\n",
    "        | \\.\\.\\.        # ellipsis\n",
    "        | [][.,;”’?():-_%#’]    # separate tokens\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'book', 'is', 'interesting', '.']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', ',', 'but', 'with', '10%', 'off', '.']\n"
     ]
    }
   ],
   "source": [
    "# the nltk regular expression tokenizer compiles the re pattern, applies it to the text\n",
    "#  and uses the matching groups to return a list of only the matched tokens\n",
    "print(nltk.regexp_tokenize(shorttext, pattern))\n",
    "print(nltk.regexp_tokenize(specialtext, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', ',', 'but', 'with', '10', '%', 'off', '.']\n"
     ]
    }
   ],
   "source": [
    "# compare with built-in word tokenizer\n",
    "print(nltk.word_tokenize(specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenizer for Twitter derived tweetmotif from the ARK, developed at CMU\n",
    "tweetPattern = r''' (?x)\t# set flag to allow verbose regexps\n",
    "      (?:https?://|www)\\S+      # simple URLs\n",
    "      | (?::-\\)|;-\\))\t\t# small list of emoticons\n",
    "      | &(?:amp|lt|gt|quot);    # XML or HTML entity\n",
    "      | \\#\\w+                 # hashtags\n",
    "      | @\\w+                  # mentions   \n",
    "      | \\d+:\\d+               # timelike pattern\n",
    "      | \\d+\\.\\d+              # number with a decimal\n",
    "      | (?:\\d+,)+?\\d{3}(?=(?:[^,]|$))   # number with a comma\n",
    "      | (?:[A-Z]\\.)+                    # simple abbreviations\n",
    "      | (?:--+)               # multiple dashes\n",
    "      | \\w+(?:-\\w+)*          # words with internal hyphens or apostrophes\n",
    "      | ['\\\".?!,:;/]+         # special characters\n",
    "      '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example tweets\n",
    "tweet1 = \"@natalieohayre I agree #hc09 needs reform- but not by crooked politicians who r clueless about healthcare! #tcot #fishy NO GOV'T TAKEOVER!\"\n",
    "tweet2 = \"To Sen. Roland Burris: Affordable, quality health insurance can't wait http://bit.ly/j63je #hc09 #IL #60660\"\n",
    "tweet3 = \"RT @karoli: RT @Seriou: .@whitehouse I will stand w/ Obama on #healthcare,  I trust him. #p2 #tlot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@natalieohayre', 'I', 'agree', '#hc09', 'needs', 'reform', 'but', 'not', 'by', 'crooked', 'politicians', 'who', 'r', 'clueless', 'about', 'healthcare', '!', '#tcot', '#fishy', 'NO', 'GOV', \"'\", 'T', 'TAKEOVER', '!']\n",
      "['To', 'Sen', '.', 'Roland', 'Burris', ':', 'Affordable', ',', 'quality', 'health', 'insurance', 'can', \"'\", 't', 'wait', 'http://bit.ly/j63je', '#hc09', '#IL', '#60660']\n",
      "['RT', '@karoli', ':', 'RT', '@Seriou', ':', '.', '@whitehouse', 'I', 'will', 'stand', 'w', '/', 'Obama', 'on', '#healthcare', ',', 'I', 'trust', 'him', '.', '#p2', '#tlot']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.regexp_tokenize(tweet1,tweetPattern))\n",
    "print(nltk.regexp_tokenize(tweet2,tweetPattern))\n",
    "print(nltk.regexp_tokenize(tweet3,tweetPattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@natalieohayre', 'I', 'agree', '#hc09', 'needs', 'reform', '-', 'but', 'not', 'by', 'crooked', 'politicians', 'who', 'r', 'clueless', 'about', 'healthcare', '!', '#tcot', '#fishy', 'NO', \"GOV'T\", 'TAKEOVER', '!']\n"
     ]
    }
   ],
   "source": [
    "# NLTK built-in tokenizer (more detailed version from TweetMotif)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "ttokenizer = TweetTokenizer()\n",
    "print(ttokenizer.tokenize(tweet1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Black ', 'and ', 'Mrs.', 'Brown ', 'attended ', 'the ', 'lecture ', 'by ', 'Dr.', 'Gray,', 'but ', 'Gov.', 'White ', \"wasn'\", 't ', 'there.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"Mr. Black and Mrs. Brown attended the lecture by Dr. Gray, but Gov. White wasn't there.\"\n",
    "print(nltk.regexp_tokenize(sent, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lab Exercise\n",
    "Choose one of the following, i.e. work with either the regular pattern or the tweet pattern in the\n",
    "tokenizer.\n",
    "1. Run the regexp tokenizer with the regular pattern on the sentence “Mr. Black and Mrs. Brown attended the lecture by Dr. Gray, but Gov. White wasn’t there.”\n",
    "b. Design and add the pattern of this tokenizer so that words with a single\n",
    "apostrophe, such as “wasn’t” are taken as a single token.\n",
    "OR\n",
    "2. Run the regexp tokenizer with the tweet pattern on the three example tweets.\n",
    "a. Design and add a line to the pattern of this tokenizer so that titles like “Sen.” and\n",
    "“Rep.” are tokenized as having the dot inside the token. Test and add some other\n",
    "titles to your list of titles.\n",
    "b. Design and add to the pattern of this tokenizer so that words with a single\n",
    "apostrophe, such as “can’t” are taken as a single token.\n",
    "c. Design and add to the pattern of this tokenizer so that the abbreviation “w/” is\n",
    "taken as a single token.\n",
    "Choose at least one of your tokenizer solutions and post your revised pattern to the Assignment\n",
    "in Blackboard for Week 4, with a short example text that demonstrates its effect. Mention any\n",
    "examples that you think of that need additional regular expressions to be tokenized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Black ', 'and ', 'Mrs.', 'Brown ', 'attended ', 'the ', 'lecture ', 'by ', 'Dr.', 'Gray,', 'but ', 'Gov.', 'White ', 'wasn’', 't ', 'there.']\n"
     ]
    }
   ],
   "source": [
    "#Run the regexp tokenizer with the regular pattern on \n",
    "#the sentence “Mr. Black and Mrs. Brown attended the lecture \n",
    "#by Dr. Gray, but Gov. White wasn’t there.”\n",
    "\n",
    "tmp = \"Mr. Black and Mrs. Brown attended the lecture by Dr. Gray, but Gov. White wasn’t there.\"\n",
    "\n",
    "'''1. Design and add a line to the pattern of this tokenizer so that titles like “Mr.” \n",
    "are tokenized as having the dot inside the token. Test and add some other titles to your list of titles.\n",
    "'''\n",
    "pattern = r''' (?x) \t# set flag to allow verbose regexps\n",
    "         \\w+. #to allow  .\n",
    "        | (?:[A-Z]\\.)+    # abbreviations, e.g. U.S.A.\n",
    "        | \\$?\\d+(?:\\.\\d+)?%?    # currency and percentages, $12.40, 50%\n",
    "        | \\w+(?:-\\w+)*  # words with internal hyphens\n",
    "        | \\.\\.\\.        # ellipsis\n",
    "        | [][.,;”’?():-_%#’]    # separate tokens\n",
    "        '''\n",
    "#print(nltk.regexp_tokenize(tmp, pattern))\n",
    "\n",
    "'''b. Design and add the pattern of this tokenizer so that words with a single\n",
    "apostrophe, such as “wasn't” are taken as a single token.\n",
    "'''\n",
    "\n",
    "pattern2 = r''' (?x) \t# set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+    # abbreviations, e.g. U.S.A.\n",
    "        | \\$?\\d+(?:\\.\\d+)?%?    # currency and percentages, $12.40, 50%\n",
    "        | \\w+. #to allow\n",
    "        | \\w+(?:’\\w+)*  # words with internal apostrophe\n",
    "        | \\w+(?:-\\w+)*  # words with internal hyphens\n",
    "        | \\.\\.\\.        # ellipsis\n",
    "        | [][.,;”’?():-_%#’]    # separate tokens\n",
    "\n",
    "        '''\n",
    "                              \n",
    "print(nltk.regexp_tokenize(tmp, pattern2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
