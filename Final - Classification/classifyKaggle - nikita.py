'''  This program shell reads phrase data for the kaggle phrase sentiment classification problem.  The input to the program is the path to the kaggle directory "corpus" and a limit number.  The program reads all of the kaggle phrases, and then picks a random selection of the limit number.  It creates a "phrasedocs" variable with a list of phrases consisting of a pair    with the list of tokenized words from the phrase and the label number from 1 to 4  It prints a few example phrases.  In comments, it is shown how to get word lists from the two sentiment lexicons:      subjectivity and LIWC, if you want to use them in your features  Your task is to generate features sets and train and test a classifier.  Usage:  python classifyKaggle.py  <corpus directory path> <limit number>'''# open python and nltk packages needed for processingimport osimport sysimport randomimport nltkimport refrom nltk.corpus import stopwordsimport NLTK_cross_validation_evaluationimport sentiment_read_subjectivity# initialize the positive, neutral and negative word lists(positivelist, neutrallist, negativelist) = sentiment_read_subjectivity.read_subjectivity_three_types("C:/Users/Nikita/Desktop/IST 664/subjclueslen1-HLTEMNLP05.tff/subjclueslen1-HLTEMNLP05.tff")SL = sentiment_read_subjectivity.readSubjectivity("C:/Users/Nikita/Desktop/IST 664/subjclueslen1-HLTEMNLP05.tff/subjclueslen1-HLTEMNLP05.tff")#import sentiment_read_LIWC_pos_neg_words# initialize positve and negative word prefix lists from LIWC #   note there is another function isPresent to test if a word's prefix is in the list#(poslist, neglist) = sentiment_read_LIWC_pos_neg_words.read_words()# define a feature definition function heredef document_features(document, word_features):    document_words = set(document)    features = {}    for word in word_features:        features['contains({})'.format(word)] = (word in document_words)    return featuresdef SL_features(document, word_features, SL):    document_words = set(document)    features = {}    for word in word_features:        features['contains({})'.format(word)] = (word in document_words)    # count variables for the 4 classes of subjectivity    weakPos = 0    strongPos = 0    weakNeg = 0    strongNeg = 0    for word in document_words:        if word in SL:            strength, posTag, isStemmed, polarity = SL[word]            if strength == 'weaksubj' and polarity == 'positive':                weakPos += 1            if strength == 'strongsubj' and polarity == 'positive':                strongPos += 1            if strength == 'weaksubj' and polarity == 'negative':                weakNeg += 1            if strength == 'strongsubj' and polarity == 'negative':                strongNeg += 1            features['positivecount'] = weakPos + (2 * strongPos)            features['negativecount'] = weakNeg + (2 * strongNeg)    return features# function to read kaggle training file, train and test a classifier def processkaggle(dirPath,limitStr):  # convert the limit argument from a string to an int  limit = int(limitStr)    os.chdir(dirPath)    f = open('./train.tsv', 'r')  # loop over lines in the file and use the first limit of them  phrasedata = []  for line in f:    # ignore the first line starting with Phrase and read all lines    if (not line.startswith('Phrase')):      # remove final end of line character      line = line.strip()      # each line has 4 items separated by tabs      # ignore the phrase and sentence ids, and keep the phrase and sentiment      phrasedata.append(line.split('\t')[2:4])    # pick a random sample of length limit because of phrase overlapping sequences  random.shuffle(phrasedata)  phraselist = phrasedata[:limit]  print('Read', len(phrasedata), 'phrases, using', len(phraselist), 'random phrases')  for phrase in phraselist[:10]:    print (phrase)    # create list of phrase documents as (list of words, label)  phrasedocs = []  # add all the phrases  for phrase in phraselist:    tokens = nltk.word_tokenize(phrase[0])    # possibly filter tokens    stopwords = nltk.corpus.stopwords.words('english')    stoppedtokens = [w for w in tokens if not w in stopwords]    # function that takes a word and returns true if it consists only    #   of non-alphabetic characters  (assumes import re)    def alpha_filter(w):      # pattern to match word of non-alphabetical characters      pattern = re.compile('^[^a-z]+$')      if (pattern.match(w)):        return True      else:        return False    specialstoppedtokens = [w for w in stoppedtokens if not alpha_filter(w)]    phrasedocs.append((specialstoppedtokens, int(phrase[1])))  # print a few  for phrase in phrasedocs[:10]:    print (phrase)  # continue as usual to get all words and create word features  all_words_list = [word for (sent, cat) in phrasedocs for word in sent]  all_words = nltk.FreqDist(all_words_list)  # get the 2000 most frequently appearing keywords in the corpus  word_items = all_words.most_common(2000)  word_features = [word for (word, count) in word_items]  print(word_features[:50])  # feature sets from a feature definition function  # get features sets for a document, including keyword features and category feature  #featuresets = [(document_features(d, word_features), c) for (d, c) in phrasedocs]  SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in phrasedocs]  #train classifier and show performance in cross-validation  # training using naive Baysian classifier, training set is approximately 90% of data  n=90  train_set, test_set = SL_featuresets[n:], SL_featuresets[:n]  classifier = nltk.NaiveBayesClassifier.train(train_set)  # evaluate the accuracy of the classifier  print(nltk.classify.accuracy(classifier, test_set))  # the accuracy result may vary since we randomized the documents  num_folds = 5  # or 5  label_list = ['positivelist', 'neutrallist', 'negativelist']  NLTK_cross_validation_evaluation.cross_validate_evaluate(num_folds, SL_featuresets, label_list)"""commandline interface takes a directory name with kaggle subdirectory for train.tsv   and a limit to the number of kaggle phrases to useIt then processes the files and trains a kaggle movie review sentiment classifier."""if __name__ == '__main__':    if (len(sys.argv) != 3):        print ('usage: classifyKaggle.py <corpus-dir> <limit>')        sys.exit(0)    processkaggle(sys.argv[1], sys.argv[2])