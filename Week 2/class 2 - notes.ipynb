{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bigrams\n",
    "\n",
    "In the text: “two great and powerful groups of nations”, the\n",
    "bigrams are “two great”, “great and”, “and powerful”, etc.\n",
    "\n",
    "The **frequency of an n-gram** is the percentage of\n",
    "times the n-gram occurs in all the n-grams of the\n",
    "corpus and could be useful in corpus statistics\n",
    "- For bigram xy: Count of bigram xy/Count of all bigrams in corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting started to process a text example\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt'] \n",
      "\n",
      "191673 \n",
      "\n",
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an']\n"
     ]
    }
   ],
   "source": [
    "# get the text of the book Emma from the Gutenberg corpus, tokenize it,\n",
    "#   and reduce the tokens to lowercase.\n",
    "print (nltk.corpus.gutenberg.fileids(),'\\n')\n",
    "file0 = nltk.corpus.gutenberg.fileids() [0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0)\n",
    "emmatokens = nltk.word_tokenize(emmatext)\n",
    "emmawords = [w.lower( ) for w in emmatokens]\n",
    "# show the number of words and print the first 110 words\n",
    "print(len(emmawords), '\\n')\n",
    "\n",
    "print(emmawords[ :110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", \t 12016\n",
      ". \t 6355\n",
      "the \t 5198\n",
      "to \t 5179\n",
      "and \t 4875\n",
      "of \t 4284\n",
      "i \t 3164\n",
      "a \t 3124\n",
      "-- \t 3100\n",
      "it \t 2500\n",
      "'' \t 2452\n",
      "her \t 2448\n",
      "was \t 2396\n",
      "; \t 2353\n",
      "she \t 2336\n",
      "not \t 2279\n",
      "in \t 2173\n",
      "be \t 1970\n",
      "you \t 1962\n",
      "he \t 1806\n",
      "that \t 1804\n",
      "`` \t 1735\n",
      "had \t 1623\n",
      "but \t 1439\n",
      "as \t 1436\n",
      "for \t 1346\n",
      "have \t 1319\n",
      "is \t 1241\n",
      "with \t 1215\n",
      "very \t 1202\n"
     ]
    }
   ],
   "source": [
    "#create a frequency distribution of the words\n",
    "ndist = FreqDist(emmawords)\n",
    "nitems = ndist.most_common(30)\n",
    "for item in nitems:\n",
    "    print (item[0], '\\t',item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191673   192427\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emmawords</th>\n",
       "      <th>emmawords2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[</td>\n",
       "      <td>[</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emma</td>\n",
       "      <td>emma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jane</td>\n",
       "      <td>jane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>austen</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1816</td>\n",
       "      <td>1816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>]</td>\n",
       "      <td>]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>volume</td>\n",
       "      <td>volume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chapter</td>\n",
       "      <td>chapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>emma</td>\n",
       "      <td>emma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>woodhouse</td>\n",
       "      <td>woodhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>handsome</td>\n",
       "      <td>handsome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>clever</td>\n",
       "      <td>clever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rich</td>\n",
       "      <td>rich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>comfortable</td>\n",
       "      <td>comfortable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>home</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>happy</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>disposition</td>\n",
       "      <td>disposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>seemed</td>\n",
       "      <td>seemed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>unite</td>\n",
       "      <td>unite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>some</td>\n",
       "      <td>some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>blessings</td>\n",
       "      <td>blessings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>existence</td>\n",
       "      <td>existence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>;</td>\n",
       "      <td>;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>had</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>lived</td>\n",
       "      <td>lived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>nearly</td>\n",
       "      <td>nearly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>twenty-one</td>\n",
       "      <td>twenty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>years</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>in</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>the</td>\n",
       "      <td>years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>world</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>with</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      emmawords   emmawords2\n",
       "0             [            [\n",
       "1          emma         emma\n",
       "2            by           by\n",
       "3          jane         jane\n",
       "4        austen       austen\n",
       "5          1816         1816\n",
       "6             ]            ]\n",
       "7        volume       volume\n",
       "8             i            i\n",
       "9       chapter      chapter\n",
       "10            i            i\n",
       "11         emma         emma\n",
       "12    woodhouse    woodhouse\n",
       "13            ,            ,\n",
       "14     handsome     handsome\n",
       "15            ,            ,\n",
       "16       clever       clever\n",
       "17            ,            ,\n",
       "18          and          and\n",
       "19         rich         rich\n",
       "20            ,            ,\n",
       "21         with         with\n",
       "22            a            a\n",
       "23  comfortable  comfortable\n",
       "24         home         home\n",
       "25          and          and\n",
       "26        happy        happy\n",
       "27  disposition  disposition\n",
       "28            ,            ,\n",
       "29       seemed       seemed\n",
       "30           to           to\n",
       "31        unite        unite\n",
       "32         some         some\n",
       "33           of           of\n",
       "34          the          the\n",
       "35         best         best\n",
       "36    blessings    blessings\n",
       "37           of           of\n",
       "38    existence    existence\n",
       "39            ;            ;\n",
       "40          and          and\n",
       "41          had          had\n",
       "42        lived        lived\n",
       "43       nearly       nearly\n",
       "44   twenty-one       twenty\n",
       "45        years            -\n",
       "46           in          one\n",
       "47          the        years\n",
       "48        world           in\n",
       "49         with          the"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Different version of the words using \"words\" function\n",
    "emmawords2 = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "emmawords2lowercase = [w.lower() for w in emmawords2]\n",
    "\n",
    "\n",
    "#Do you think the word lists that emmawords and emmawords2lowercase contain should be\n",
    "#identical? Try this out and see if this is what you expected: they are different\n",
    "print(len(emmawords),' ',len(emmawords2lowercase))\n",
    "\n",
    "#Now try printing some of the words. What difference do you see?\n",
    "tmp=np.array(emmawords[:50])\n",
    "tmp2=np.array(emmawords2lowercase[:50])\n",
    "d = {'emmawords': tmp, 'emmawords2': tmp2}\n",
    "display(pd.DataFrame(data=d))\n",
    "#emmawords2 separates twenty-one into 'twenty', '-' and 'one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "161456\n",
      "['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', 'indulgent', 'father', 'and', 'had', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses']\n"
     ]
    }
   ],
   "source": [
    "#remove all the tokens that have only special characters\n",
    "#this regular expression pattern matches any word that contains all non-alphabetical\n",
    "# lower-case characters\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "pattern = re.compile('^[^a-z]+$') \n",
    "#any string that are contrainslower-case letter will not be matched\n",
    "\n",
    "def alpha_filter(w):\n",
    "# pattern to match a word of non-alphabetical characters\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if (pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(alpha_filter(\"aZa\"))\n",
    "print(alpha_filter(\"ZZ\"))\n",
    "\n",
    "\n",
    "#Apply the new function to emmawords to include only those that don’t match the filter:\n",
    "alphaemmawords = [w for w in emmawords if not alpha_filter(w)]\n",
    "print(len(alphaemmawords))\n",
    "print(alphaemmawords[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74093\n",
      "[('mr.', 1089), (\"'s\", 866), ('emma', 855), ('could', 836), ('would', 818), ('mrs.', 668), ('miss', 597), ('must', 566), ('harriet', 496), ('much', 484), ('said', 483), ('one', 447), ('every', 434), ('weston', 430), ('thing', 394), ('think', 383), ('elton', 378), ('well', 375), ('knightley', 373), ('little', 359), ('never', 358), ('know', 335), ('might', 325), ('good', 313), ('say', 310), ('woodhouse', 308), ('jane', 299), ('quite', 282), ('time', 275), ('great', 263)]\n"
     ]
    }
   ],
   "source": [
    "#remove some of the common words that appear \n",
    "#with great frequency.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stoppedemmawords=[]\n",
    "for word in alphaemmawords:\n",
    "    if word not in stopwords:\n",
    "        stoppedemmawords.append(word)\n",
    "#stoppedemmawords = [w for w in alphaemmawords if not w in stopwords]\n",
    "print(len(stoppedemmawords))\n",
    "        \n",
    "#Now we can remake our frequency distribution with our new filtered word list.\n",
    "emmadist = FreqDist(stoppedemmawords)\n",
    "emmaitems = emmadist.most_common(30)\n",
    "for item in emmaitems:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[]\n",
    "Used to indicate a set of characters. In a set:\n",
    "\n",
    "- Characters can be listed individually, e.g. [amk] will match 'a', 'm', or 'k'.\n",
    "- Ranges of characters can be indicated by giving two characters and separating them by a '-', for example [a-z] will match any lowercase ASCII letter, [0-5][0-9] will match all the two-digits numbers from 00 to 59, and [0-9A-Fa-f] will match any hexadecimal digit. If - is escaped (e.g. [a\\-z]) or if it’s placed as the first or last character (e.g. [a-]), it will match a literal '-'.\n",
    "- Special characters lose their special meaning inside sets. For example, [(+*)] will match any of the literal characters '(', '+', '*', or ')'.\n",
    "- Character classes such as \\w or \\S (defined below) are also accepted inside a set, although the characters they match depends on whether LOCALE or UNICODE mode is in force.\n",
    "- Characters that are not within a range can be matched by complementing the set. If the first character of the set is '^', all the characters that are not in the set will be matched. For example, [^5] will match any character except '5', and [^^] will match any character except '^'. ^ has no special meaning if it’s not the first character in the set.\n",
    "- To match a literal ']' inside a set, precede it with a backslash, or place it at the beginning of the set. For example, both [()[\\]{}] and []()[{}] will both match a parenthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'emma'), ('emma', 'by'), ('by', 'jane'), ('jane', 'austen'), ('austen', '1816'), ('1816', ']'), (']', 'volume'), ('volume', 'i'), ('i', 'chapter'), ('chapter', 'i'), ('i', 'emma'), ('emma', 'woodhouse'), ('woodhouse', ','), (',', 'handsome'), ('handsome', ','), (',', 'clever'), ('clever', ','), (',', 'and'), ('and', 'rich'), ('rich', ',')]\n",
      "<class 'list'>\n",
      "(('to', 'be'), 0.0031512002212100818)\n",
      "(('of', 'the'), 0.002916425370292112)\n",
      "(('in', 'the'), 0.0023216624146332556)\n",
      "(('it', 'was'), 0.002306010757905391)\n",
      "(('i', 'am'), 0.0020451498124409804)\n",
      "(('she', 'had'), 0.0017321166778836872)\n",
      "(('she', 'was'), 0.0017112478022465346)\n",
      "(('had', 'been'), 0.001601686205151482)\n",
      "(('it', 'is'), 0.0015442967971493115)\n",
      "(('i', 'have'), 0.0014660385135099885)\n",
      "(('could', 'not'), 0.0014503868567821237)\n",
      "(('mr.', 'knightley'), 0.0014138663244171062)\n",
      "(('of', 'her'), 0.0013564769164149358)\n",
      "(('mrs.', 'weston'), 0.0012834358516849009)\n",
      "(('have', 'been'), 0.0012573497571384598)\n",
      "(('he', 'had'), 0.0012521325382291715)\n",
      "(('to', 'the'), 0.001236480881501307)\n",
      "(('do', 'not'), 0.0012208292247734424)\n",
      "(('and', 'the'), 0.00116865703568056)\n",
      "(('he', 'was'), 0.0011582225978619836)\n",
      "(('would', 'be'), 0.0011217020654969662)\n",
      "(('mr.', 'elton'), 0.0011008331898598133)\n",
      "(('such', 'a'), 0.0010434437818576429)\n",
      "(('a', 'very'), 0.0010330093440390664)\n",
      "(('of', 'his'), 0.0009912715927647608)\n",
      "(('that', 'she'), 0.0009599682793090315)\n",
      "(('to', 'have'), 0.0009599682793090315)\n",
      "(('to', 'her'), 0.0009599682793090315)\n",
      "(('did', 'not'), 0.0009547510603997434)\n",
      "(('and', 'i'), 0.0009443166225811669)\n",
      "\n",
      " filtered out the stopwords\n",
      "\n",
      "(('mr.', 'knightley'), 0.0014138663244171062)\n",
      "(('mrs.', 'weston'), 0.0012834358516849009)\n",
      "(('mr.', 'elton'), 0.0011008331898598133)\n",
      "(('miss', 'woodhouse'), 0.0008764927767604201)\n",
      "(('mr.', 'weston'), 0.0008243205876675379)\n",
      "(('frank', 'churchill'), 0.000751279522937503)\n",
      "(('mrs.', 'elton'), 0.0007304106473003501)\n",
      "(('mr.', 'woodhouse'), 0.0006834556771167561)\n",
      "(('every', 'thing'), 0.0006469351447517387)\n",
      "(('miss', 'fairfax'), 0.0006365007069331622)\n",
      "(('miss', 'bates'), 0.0005791112989309918)\n",
      "(('every', 'body'), 0.0005686768611124154)\n",
      "(('jane', 'fairfax'), 0.0005425907665659743)\n",
      "(('harriet', \"'s\"), 0.0004486808261987865)\n",
      "(('young', 'man'), 0.0004330291694709218)\n",
      "(('emma', \"'s\"), 0.0003860741992873279)\n",
      "(('great', 'deal'), 0.0003339020101944457)\n",
      "(('elton', \"'s\"), 0.00032346757237586933)\n",
      "(('emma', 'could'), 0.0003182503534665811)\n",
      "(('said', 'emma'), 0.00030781591564800465)\n",
      "(('miss', 'smith'), 0.00029738147782942825)\n",
      "(('john', 'knightley'), 0.0002869470400108518)\n",
      "(('mrs.', 'goddard'), 0.0002712953832829872)\n",
      "(('dare', 'say'), 0.00026607816437369895)\n",
      "(('mr.', 'frank'), 0.00025564372655512255)\n",
      "(('miss', 'taylor'), 0.0002452092887365461)\n",
      "(('weston', \"'s\"), 0.00022955763200868144)\n",
      "(('mrs.', 'churchill'), 0.00020347153746224037)\n",
      "(('said', 'mr.'), 0.00019825431855295217)\n",
      "(('mr.', 'martin'), 0.00019303709964366394)\n"
     ]
    }
   ],
   "source": [
    "#Bigram Frequency Distributions\n",
    "#Another way to look for interesting characterizations of a corpus is to look at pairs of\n",
    "#words that are frequently collocated, that is, they occur in a sequence called a bigram.\n",
    "\n",
    "#look at the bigrams that can be defined.\n",
    "emmabigrams = list(nltk.bigrams(emmawords))\n",
    "print(emmabigrams[:20])\n",
    "\n",
    "#To start using bigrams, we import the collocation finder module.\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "#you must us the entire list of emmawords before any filtering or the raw\n",
    "#bigrams will not be correct. Start with all the words and then run the filters in the bigram\n",
    "#finder.\n",
    "\n",
    "#The finder then allows us to call other functions to filter the bigrams that it collected and to give scores to the\n",
    "#bigrams.\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(emmawords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "print(type(scored))\n",
    "first = scored[0]\n",
    "\n",
    "# for bscore in scored[:30]:\n",
    "#     print (bscore)\n",
    "    \n",
    "#Apply alpha_filter on the finder\n",
    "finder.apply_word_filter(alpha_filter) #filter out the shit we don't want\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)\n",
    "\n",
    "print('\\n',\"filtered out the stopwords:\\n\")\n",
    "\n",
    "#filter out the stopwords\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((',', 'and'), 0.00981880598728042)\n",
      "(('.', \"''\"), 0.0060363222780464645)\n",
      "((\"''\", '``'), 0.005003312934007398)\n",
      "((';', 'and'), 0.004523328794352882)\n",
      "(('to', 'be'), 0.0031512002212100818)\n",
      "((',', \"''\"), 0.0030468558430243172)\n",
      "(('.', 'i'), 0.0029738147782942823)\n",
      "((',', 'i'), 0.0029685975593849944)\n",
      "(('of', 'the'), 0.002916425370292112)\n",
      "(('in', 'the'), 0.0023216624146332556)\n",
      "(('it', 'was'), 0.002306010757905391)\n",
      "((';', 'but'), 0.0022277524742660678)\n",
      "(('.', '``'), 0.0021703630662638974)\n",
      "(('.', 'she'), 0.002154711409536033)\n",
      "(('i', 'am'), 0.0020451498124409804)\n",
      "((',', 'that'), 0.0018781988073437574)\n",
      "(('!', '--'), 0.001794723304795146)\n",
      "(('--', 'and'), 0.0017425511157022637)\n",
      "(('she', 'had'), 0.0017321166778836872)\n",
      "(('she', 'was'), 0.0017112478022465346)\n"
     ]
    }
   ],
   "source": [
    "#another filter that would remove words that only occurred with a \n",
    "#frequency over some minimum threshold \n",
    "\n",
    "finder2 = BigramCollocationFinder.from_words(emmawords)\n",
    "finder2.apply_freq_filter(2)\n",
    "#Removes candidate ngrams which have frequency less than min_freq.\n",
    "\n",
    "scored = finder2.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:20]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((\"''\", '``'), 0.005003312934007398)\n",
      "(('to', 'be'), 0.0031512002212100818)\n",
      "(('of', 'the'), 0.002916425370292112)\n",
      "(('in', 'the'), 0.0023216624146332556)\n",
      "(('it', 'was'), 0.002306010757905391)\n",
      "(('--', 'and'), 0.0017425511157022637)\n",
      "(('she', 'had'), 0.0017321166778836872)\n",
      "(('she', 'was'), 0.0017112478022465346)\n",
      "(('had', 'been'), 0.001601686205151482)\n",
      "(('it', 'is'), 0.0015442967971493115)\n",
      "(('could', 'not'), 0.0014503868567821237)\n",
      "(('mr.', 'knightley'), 0.0014138663244171062)\n",
      "((\"''\", 'said'), 0.0013825630109613768)\n",
      "(('``', 'i'), 0.0013616941353242241)\n",
      "(('of', 'her'), 0.0013564769164149358)\n",
      "(('--', 'i'), 0.0013408252596870712)\n",
      "(('mrs.', 'weston'), 0.0012834358516849009)\n",
      "(('have', 'been'), 0.0012573497571384598)\n",
      "(('he', 'had'), 0.0012521325382291715)\n",
      "(('to', 'the'), 0.001236480881501307)\n"
     ]
    }
   ],
   "source": [
    "#filter out the words whose length is small\n",
    "finder2.apply_ngram_filter(lambda w1, w2: len(w1) < 2)\n",
    "scored = finder2.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:20]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information and other scorers\n",
    "N-Gram probabilities predict the next word, Mutual\n",
    "Information computes probability of two words\n",
    "occurring in sequence\n",
    "- Given a pair of words, compares probability that the two occur together as a joint event to the probability they occur individually & that their co-occurrences are simply the result of chance\n",
    "- The more strongly connected 2 items are, the higher will be their MI value\n",
    "\n",
    "\n",
    "• Based on work of Church & Hanks (1990), generalizing MI from\n",
    "information theory to apply to words in sequence\n",
    "– They used terminology Association Ratio\n",
    "- P(x) and P(y) are estimated by the number of\n",
    "observations of x and y in a corpus and normalized by N,\n",
    "the size of the corpus\n",
    "- P(x,y) is the number of times that x is followed by y in a\n",
    "window of w words\n",
    "- Mutual Information score (also sometimes called PMI,\n",
    "Pointwise Mutual Information):\n",
    "PMI (x,y) = log2 ( P(x,y) / P(x) P(y) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('26th', 'ult.'), 17.54828760064729)\n",
      "(('_______', 'regiment'), 17.54828760064729)\n",
      "(('_a_', '_source_'), 17.54828760064729)\n",
      "(('_amor_', '_patriae_'), 17.54828760064729)\n",
      "(('_and_', '_misery_'), 17.54828760064729)\n",
      "(('_any_', '_thing_'), 17.54828760064729)\n",
      "(('_be_', '_a_'), 17.54828760064729)\n",
      "(('_caro_', '_sposo_'), 17.54828760064729)\n",
      "(('_dissolved_', '_it_.'), 17.54828760064729)\n",
      "(('_great_', '_way_'), 17.54828760064729)\n",
      "(('_most_', '_precious_'), 17.54828760064729)\n",
      "(('_precious_', '_treasures_'), 17.54828760064729)\n",
      "(('_repentance_', '_and_'), 17.54828760064729)\n",
      "(('_rev._', '_philip_'), 17.54828760064729)\n",
      "(('_robin_', '_adair_'), 17.54828760064729)\n",
      "(('_small_', 'half-glass'), 17.54828760064729)\n",
      "(('_with_', '_time_'), 17.54828760064729)\n",
      "(('`our', 'lot'), 17.54828760064729)\n",
      "(('adequate', 'restoratives'), 17.54828760064729)\n",
      "(('austen', '1816'), 17.54828760064729)\n",
      "(('baronne', \"d'almane\"), 17.54828760064729)\n",
      "(('base', 'aspersion'), 17.54828760064729)\n",
      "(('bulky', 'forms'), 17.54828760064729)\n",
      "(('christened', 'catherine'), 17.54828760064729)\n",
      "(('clear-sighted', 'goodwill.'), 17.54828760064729)\n",
      "(('coarser', 'featured'), 17.54828760064729)\n",
      "(('comtesse', \"d'ostalis\"), 17.54828760064729)\n",
      "(('dated', 'sept.'), 17.54828760064729)\n",
      "(('daughter-in-law', 'elect'), 17.54828760064729)\n",
      "(('de', 'genlis'), 17.54828760064729)\n",
      "\n",
      "\n",
      "(('d', \"'ye\"), 14.96332509992613)\n",
      "(('sore', 'throat'), 14.088855982009989)\n",
      "(('brunswick', 'square'), 13.951352458260054)\n",
      "(('william', 'larkins'), 13.08885598200999)\n",
      "(('baked', 'apples'), 12.96332509992613)\n",
      "(('box', 'hill'), 12.73521902739529)\n",
      "(('sixteen', 'miles'), 12.612827852841999)\n",
      "(('maple', 'grove'), 12.59409129026041)\n",
      "(('hair', 'cut'), 12.062860773477047)\n",
      "(('south', 'end'), 11.963325099926132)\n",
      "(('colonel', 'campbell'), 11.411391399592448)\n",
      "(('protest', 'against'), 11.346653739477636)\n",
      "(('robert', 'martin'), 11.093092974896459)\n",
      "(('five', 'couple'), 10.840928468566405)\n",
      "(('vast', 'deal'), 10.761691238756482)\n",
      "(('ready', 'wit'), 10.65145066970269)\n",
      "(('donwell', 'abbey'), 10.518540257253235)\n",
      "(('musical', 'society'), 10.508271921799409)\n",
      "(('infinitely', 'superior'), 10.229970759312303)\n",
      "(('married', 'women'), 10.056434504317613)\n",
      "(('five', 'minutes'), 10.031871251277801)\n",
      "(('three', 'months'), 9.96332509992613)\n",
      "(('years', 'ago'), 9.95666136964512)\n",
      "(('depend', 'upon'), 9.931738756868299)\n",
      "(('ten', 'minutes'), 9.866170835697215)\n",
      "(('sat', 'down'), 9.794513718794526)\n",
      "(('hurrying', 'away'), 9.602258611131811)\n",
      "(('few', 'moments'), 9.557332740250294)\n",
      "(('few', 'minutes'), 9.414374786408253)\n",
      "(('lovely', 'woman'), 9.410784076897352)\n"
     ]
    }
   ],
   "source": [
    "#In NLTK, the mutual information score is given by a function for Pointwise Mutual Information,\n",
    "#where this is the version without the window.\n",
    "finder3 = BigramCollocationFinder.from_words(emmawords)\n",
    "scored = finder3.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)\n",
    "\n",
    "print ('\\n')\n",
    "#It is recommended to run the PMI scorer with a minimum frequency of 5, which will make more sense on very large documents.\n",
    "finder3.apply_freq_filter(5)\n",
    "scored = finder3.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(,, and)</td>\n",
       "      <td>0.012801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(ham, .)</td>\n",
       "      <td>0.009277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(my, lord)</td>\n",
       "      <td>0.004817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(., i)</td>\n",
       "      <td>0.004157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(,, that)</td>\n",
       "      <td>0.003744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(,, i)</td>\n",
       "      <td>0.002808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(king, .)</td>\n",
       "      <td>0.002643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(hor, .)</td>\n",
       "      <td>0.002615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(,, the)</td>\n",
       "      <td>0.002560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(,, to)</td>\n",
       "      <td>0.002175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(,, as)</td>\n",
       "      <td>0.002037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(., enter)</td>\n",
       "      <td>0.002010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(in, the)</td>\n",
       "      <td>0.002010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(lord, ,)</td>\n",
       "      <td>0.002010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(,, but)</td>\n",
       "      <td>0.001844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(,, my)</td>\n",
       "      <td>0.001844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(., what)</td>\n",
       "      <td>0.001844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(?, ham)</td>\n",
       "      <td>0.001707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(qu, .)</td>\n",
       "      <td>0.001707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(laer, .)</td>\n",
       "      <td>0.001652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bigrams      freq\n",
       "0     (,, and)  0.012801\n",
       "1     (ham, .)  0.009277\n",
       "2   (my, lord)  0.004817\n",
       "3       (., i)  0.004157\n",
       "4    (,, that)  0.003744\n",
       "5       (,, i)  0.002808\n",
       "6    (king, .)  0.002643\n",
       "7     (hor, .)  0.002615\n",
       "8     (,, the)  0.002560\n",
       "9      (,, to)  0.002175\n",
       "10     (,, as)  0.002037\n",
       "11  (., enter)  0.002010\n",
       "12   (in, the)  0.002010\n",
       "13   (lord, ,)  0.002010\n",
       "14    (,, but)  0.001844\n",
       "15     (,, my)  0.001844\n",
       "16   (., what)  0.001844\n",
       "17    (?, ham)  0.001707\n",
       "18     (qu, .)  0.001707\n",
       "19   (laer, .)  0.001652"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(lord, ham)</td>\n",
       "      <td>0.002231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(hamlet, ham)</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(enter, king)</td>\n",
       "      <td>0.000466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(ha, 's)</td>\n",
       "      <td>0.000466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(enter, hamlet)</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(exeunt, enter)</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(ham, oh)</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(haue, seene)</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(let, 's)</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(lord, hamlet)</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(good, lord)</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(thou, hast)</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(enter, polonius)</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(fathers, death)</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(ham, nay)</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(lord, polon)</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(dost, thou)</td>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(good, friends)</td>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(let, vs)</td>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(ham, sir)</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              bigrams      freq\n",
       "0         (lord, ham)  0.002231\n",
       "1       (hamlet, ham)  0.000500\n",
       "2       (enter, king)  0.000466\n",
       "3            (ha, 's)  0.000466\n",
       "4     (enter, hamlet)  0.000333\n",
       "5     (exeunt, enter)  0.000333\n",
       "6           (ham, oh)  0.000333\n",
       "7       (haue, seene)  0.000333\n",
       "8           (let, 's)  0.000333\n",
       "9      (lord, hamlet)  0.000333\n",
       "10       (good, lord)  0.000300\n",
       "11       (thou, hast)  0.000300\n",
       "12  (enter, polonius)  0.000266\n",
       "13   (fathers, death)  0.000266\n",
       "14         (ham, nay)  0.000266\n",
       "15      (lord, polon)  0.000266\n",
       "16       (dost, thou)  0.000233\n",
       "17    (good, friends)  0.000233\n",
       "18          (let, vs)  0.000233\n",
       "19         (ham, sir)  0.000200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>PMI Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(th, ')</td>\n",
       "      <td>9.217978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(o, 're)</td>\n",
       "      <td>9.208910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(i'th, ')</td>\n",
       "      <td>9.143977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(any, thing)</td>\n",
       "      <td>8.798218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(fathers, death)</td>\n",
       "      <td>8.656862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(set, downe)</td>\n",
       "      <td>8.310772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(our, selues)</td>\n",
       "      <td>8.126347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(o, n't)</td>\n",
       "      <td>7.801252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(dost, thou)</td>\n",
       "      <td>7.796198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(wilt, thou)</td>\n",
       "      <td>7.770203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(drown, 'd)</td>\n",
       "      <td>7.504859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(enter, polonius)</td>\n",
       "      <td>7.417396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(ha, 's)</td>\n",
       "      <td>7.417396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(mine, owne)</td>\n",
       "      <td>7.320049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(your, lordship)</td>\n",
       "      <td>7.165721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(good, friends)</td>\n",
       "      <td>7.093432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(!, oh)</td>\n",
       "      <td>7.043330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(thou, art)</td>\n",
       "      <td>7.033238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(thou, hast)</td>\n",
       "      <td>6.863313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(as, 'twere)</td>\n",
       "      <td>6.732269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              bigrams  PMI Score\n",
       "0             (th, ')   9.217978\n",
       "1            (o, 're)   9.208910\n",
       "2           (i'th, ')   9.143977\n",
       "3        (any, thing)   8.798218\n",
       "4    (fathers, death)   8.656862\n",
       "5        (set, downe)   8.310772\n",
       "6       (our, selues)   8.126347\n",
       "7            (o, n't)   7.801252\n",
       "8        (dost, thou)   7.796198\n",
       "9        (wilt, thou)   7.770203\n",
       "10        (drown, 'd)   7.504859\n",
       "11  (enter, polonius)   7.417396\n",
       "12           (ha, 's)   7.417396\n",
       "13       (mine, owne)   7.320049\n",
       "14   (your, lordship)   7.165721\n",
       "15    (good, friends)   7.093432\n",
       "16            (!, oh)   7.043330\n",
       "17        (thou, art)   7.033238\n",
       "18       (thou, hast)   6.863313\n",
       "19       (as, 'twere)   6.732269"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nFor the exercise, I chose Hamlet from guterburg\\nHere are the frequency lists I experimented:\\nFor bigram frequency, I used:\\n1. Without a filter\\n2. Filtered out the stopwords\\nFor PMI score, I filtered out the bigram frequencies lower than 5\\nHere are the results\\n\\n'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Exercise for Week 2:\n",
    "\n",
    "For this exercise,\n",
    "- Choose a file that you want to work on, \n",
    "either one of the files from the book corpus, \n",
    "or one from the Gutenberg corpus.\n",
    "'''\n",
    "\n",
    "#Import required modules\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "\n",
    "#check what file they've got in gutenberg\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "#I will pick 'shakespeare-hamlet.txt'\n",
    "file0 = nltk.corpus.gutenberg.fileids()[-3] \n",
    "#file0 = 'shakespeare-hamlet.txt'\n",
    "\n",
    "#1. get the text with nltk.corpus.gutenberg.raw()\n",
    "hamlettext=nltk.corpus.gutenberg.raw(file0)\n",
    "\n",
    "#2. Get the tokens with nltk.word_tokenize()\n",
    "hamlettokens = nltk.word_tokenize(hamlettext)\n",
    "\n",
    "#3. Get the words by using w.lower() to lowercase the tokens\n",
    "hamletwords = [w.lower() for w in hamlettokens]\n",
    "\n",
    "#4. make a bigram finder\n",
    "#without a filter\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(hamletwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "display(pd.DataFrame(scored[:20],columns=['bigrams','freq']))\n",
    "\n",
    "\n",
    "#with alpha filter and stopwords filter \n",
    "alphahamletwords = [w for w in hamletwords if not alpha_filter(w)]\n",
    "finder = BigramCollocationFinder.from_words(alphahamletwords)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "display(pd.DataFrame(scored[:20],columns=['bigrams','freq']))\n",
    "\n",
    "\n",
    "#PMI score with min freq of 5\n",
    "finder = BigramCollocationFinder.from_words(hamletwords)\n",
    "finder.apply_freq_filter(5)\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "display(pd.DataFrame(scored[:20],columns=['bigrams','PMI Score']))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "- Make a bigram finder and experiment with whether to \n",
    "apply the filters or not . \n",
    "Run the scoring with both the raw frequency \n",
    "and the pmi scorers and compare results.\n",
    "\n",
    "To complete the exercise, choose one of your top 20 frequency lists to report to show to the\n",
    "class. Write an introductory sentence of paragraph telling what text you chose and what\n",
    "bigram filters and scorer you used. Put this and the frequency list in a discussion posting in\n",
    "the blackboard system under the Discussions tab.\n",
    "'''\n",
    "\n",
    "'''\n",
    "For the exercise, I chose Hamlet from guterburg\n",
    "Here are the frequency lists I experimented:\n",
    "For bigram frequency, I used:\n",
    "1. Without a filter\n",
    "2. Filtered out the stopwords\n",
    "For PMI score, I filtered out the bigram frequencies lower than 5\n",
    "Here are the results\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
