{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Class 1\n",
    "**Computational Linguist?**\n",
    "\n",
    "NLP is about both text analysis and text generation \n",
    "\n",
    "## Synchronic model\n",
    "\n",
    "\n",
    "\n",
    "lexical: word level\n",
    "pragmatic:\n",
    "\n",
    "\n",
    "## levels of language)\n",
    "phoeneme\n",
    "morpheme\n",
    "word\n",
    "sentence  \n",
    "text  \n",
    "world  \n",
    "\n",
    "The larger the unit of analysis, that is, the less precise our analysis gets\n",
    "\n",
    "\n",
    "- lexical semantics:\n",
    "- laxical category:\n",
    "\n",
    "phonetics rules: deals with pronunciation? Syllables?    \n",
    "phonemic rules: variations of pronounciations when words are spoken together  \n",
    "prosodic rules- fluctuation in a sentence, rhythms, volume tempo etc....  \n",
    "\n",
    "\n",
    "\n",
    "Morphological analysis: deals with the componential nature of lexical entities: (prefix, stem, suffix and stuff)\n",
    "\n",
    "Lexical level: Part-of-speech tagging tags words with specific noun, verb, adj and adv\n",
    "\n",
    "word level:\n",
    "\n",
    "syntactic level: analyze the words in sentences, grammar: rules that say how words in a language can go together\n",
    "\n",
    "sematic level:\n",
    "\n",
    "discourse: \n",
    "\n",
    "pragmatics:\n",
    "\n",
    "\n",
    "### Tokenization (or word segmentation): Decide how to separate the characters in the sentence into individual words.\n",
    "\n",
    "## Terminology for word occurrences:\n",
    "- Tokens: the total number of words\n",
    "- Distinct Tokens (sometimes called word types): number of distinct words, not counting repetitions\n",
    "- e.g. The following sentence from the Brown corpus has 16 tokens\n",
    "and 14 distinct tokens: \"They picnicked by the pool, then lay back on the grass and looked at the stars.\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "inputHidden": false,
    "new_sheet": false,
    "outputHidden": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d9ab6c256e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#Initialing nltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Word frequencies:\n",
    "1. Count the number of each token aapearing in the corpus\n",
    "2. A frequency distribution\n",
    "3. A word cloud\n",
    "'''\n",
    "\n",
    "\n",
    "#Initialing nltk\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd\n",
    "\n",
    "#nltk.download() # to download all the required shit for nltk\n",
    "brown.words()\n",
    "\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "file0 = nltk.corpus.gutenberg.fileids()[0] \n",
    "#file0 = 'austen-emma.txt'\n",
    "\n",
    "#get emma text\n",
    "emmatext=nltk.corpus.gutenberg.raw(file0)\n",
    "\n",
    "#tokenizer in nltk? is token some kind of unique word generator?\n",
    "emmatokens = nltk.word_tokenize(emmatext)\n",
    "\n",
    "#put all the tokens in lower cases\n",
    "emmawords = [w.lower() for w in emmatokens] #list comprehension\n",
    "\n",
    "#put the lower-cased tokens into a set (list without duplicated value)\n",
    "emmavocab = sorted(set(emmawords))\n",
    "\n",
    "#Create a dictionary with words as key and word frequency as values\n",
    "#[('food',3), ('lol',2)]\n",
    "fdist = FreqDist(emmawords)\n",
    "fdistkeys=list(fdist.keys())\n",
    "\n",
    "#print out the most frequent 50 words in the emmawords\n",
    "topkeys=fdist.most_common(50)\n",
    "for pair in topkeys:\n",
    "    print (pair)\n",
    "#DataFrame.from_dict(data, orient='columns', dtype=None)[source]Â¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['emma']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Do you just need an ordered sequence of items? Go for a **list**.  \n",
    "Do you just need to know whether or not you've already got a particular value, but without ordering (and you don't need to store duplicates)? Use a **set**.  \n",
    "Do you need to associate values with keys, so you can look them up efficiently (by key) later on? Use a **dictionary**.  "
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "nteract": {
   "version": "0.2.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
