{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\n",
    "Pan Chen  \n",
    "IST-664  \n",
    "HW1  \n",
    "Due 9/26/2017  \n",
    "Prof. Nancy McCracken\n",
    "**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choosing the data:  \n",
    "    \n",
    "   For this assignment, I would like to analyze and compare the first books of two contemporary literature masterpiece Series: \"Twilight\" and \"Fifty Shades of Gray\". The two highly successful franchise are actually related, as the Fifty Shades trilogy was developed from a Twilight fan fiction series originally titled Master of the Universe and published episodically on fan-fiction websites under the pen name \"Snowqueen's Icedragon\". I want to see **what do the themes and/or subject matter of Fifty Shades and Twilight have in common and what do they differ?**\n",
    "\n",
    "   Due to the very high popularities of these two books, it is easy to find them on txt format on the internet. Here I just downloaded the txt files of both books with some Google searches. \n",
    "\n",
    "   There are some limitations to my data collecting process though: the most significant one is, these documents were collected came from unreliable sources without paying(internet search engine), instead of official sources like Amazon or Kindle, so I was pretty sure I violated some copyright law. \n",
    "    \n",
    "   These documents from unoffical/unreliable sources might contain typos, imcompleteness, altering in the txt file, which might generate unreliable results. Even if there were something like typos in the document, it is hard for me to detect as I do not know if the author incorrectly misspelled the word or not. Although some of my family members did own these two books, I found it impractical to type every single word of the book into txt file for ths assignment, so I chose to get the txt files for free on the internet.\n",
    "\n",
    "   Also, when I tried to import Twilight.txt I got from the internet, the console said \"'utf-8' codec can't decode byte 0xa1 in position 1116: invalid start byte\". So I guess the txt file is not in utf-8 format. I resolved this by converting the txt file to utf-8 format with the text editior of my operating system.\n",
    "    \n",
    "   Here I will demonstrate the process of import two documents in my Python kernel in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWILIGHT \n",
      "By:Stephenie Meyer \n",
      "========================================================== \n",
      "Contents \n",
      "\n",
      "50 Shades of Grey\n",
      "\n",
      "I scowl with frustration at myself in the mirror. Damn my hair – it just won't be\n"
     ]
    }
   ],
   "source": [
    "#Import txt files\n",
    "#1. Import 50 Shades\n",
    "f = open('Fifty Shades Of Grey.txt')\n",
    "fiftytext = f.read()\n",
    "f.close()\n",
    "\n",
    "#2. Import Twilight\n",
    "f = open('Twilight.txt')\n",
    "twilighttext = f.read()\n",
    "f.close()\n",
    "\n",
    "#3. Validate if they are imported successfully\n",
    "print(twilighttext[0:100])\n",
    "print(fiftytext[0:100])\n",
    "#Hooray, they are successfully imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine the text in the documents that you chose and decide how to process the words\n",
    "\n",
    "**a) Briefly state why you chose the processing options that you did**\n",
    "\n",
    "1) Firstly, I tokenized all th words from the document\n",
    "\n",
    "2) Then I made all these tokens in lower cases..because I don't want \"Knowing\" at the beginning of a sentence and \"knowing\" in the middle of a sentence be treated like different words. \n",
    "    \n",
    "3) I made an alpha filter to filter out all the tokens that contains punctuation mark(s) only, because I was analyzing the subjects and themes of the two books, not the punctuation marks.\n",
    "\n",
    "4) I also filtered out the stopwords tokens by using the default stopwords filter from 'nltk' package, because these stop words did not contribute to the analysis of subject, as they are usually contraction words that contained little substance unlike nouns\n",
    "\n",
    "**b) Are there any problems with the word or bigram lists that you found? Could you get a better list of bigrams? How are the top 50 bigrams by frequency different from the top 50 bigrams scored by Mutual Information?**\n",
    "\n",
    "1) There is a problem with lower-casing all of the tokens: \"Grey\" as in the name \"Christian Grey\" was treated the same as \"grey\" as in the color grey, and \"Christian\" as in the name \"Chirstian Grey\" was treated as the same as \"christian\" as someone who believes in christianity.\n",
    "\n",
    "2) My initial word frequency list contained frequent tokens like \"'ll\",\"'d\",\"'s\", which would contribute little to my analysis, so I decide to add them to the stopword list and filter out these tokens. And then run the word frequency list again as well as bigrams with my new filtered tokens.\n",
    "\n",
    "**c) If you modify the stop word list, or expand the methods of filtering, describe that here.**\n",
    "\n",
    "- As I mentioned in the question above, there were some tokens that were distracting for the analysis. They were \"'s\",\"n't\",\"'m\",\"'ll\",\"'d\",\"'ll\",\"'re\",\"'ve\",\"could\",\"would\", so I filtered them out.\n",
    "\n",
    "**d)You may choose to also run top trigram lists, and include them in the analysis in part 3**\n",
    "\n",
    "- I don't really think it would be necessary for the analysis of this assignment, but I still ran one just for fun....because these two books are very entertaining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoppedfiftywords: ['shades', 'grey', 'scowl', 'frustration', 'mirror', 'damn', 'hair', 'wo', \"n't\", 'behave', 'damn', 'katherine', 'kavanagh', 'ill', 'subjecting', 'ordeal', 'studying', 'final', 'exams', 'next', 'week', 'yet', 'trying', 'brush', 'hair', 'submission', 'must', 'sleep', 'wet', 'must', 'sleep', 'wet', 'reciting', 'mantra', 'several', 'times', 'attempt', 'bring', 'control', 'brush', 'roll', 'eyes', 'exasperation', 'gaze', 'pale', 'brown-haired', 'girl', 'blue', 'eyes', 'big', 'face', 'staring', 'back', 'give', 'option', 'restrain', 'wayward', 'hair', 'ponytail', 'hope', 'look', 'semi', 'presentable', 'kate', 'roommate', 'chosen', 'today', 'days', 'succumb', 'flu', 'therefore', 'attend', 'interview', \"'d\", 'arranged', 'megaindustrialist', 'tycoon', \"'ve\", 'never', 'heard', 'student', 'newspaper', 'volunteered', 'final', 'exams', 'cram', 'one', 'essay', 'finish', \"'m\", 'supposed', 'working', 'afternoon', 'today', 'drive', 'hundred', 'sixty-five', 'miles', 'downtown', 'seattle'] \n",
      "\n",
      "stoppedtwilightwords: ['twilight', 'stephenie', 'meyer', 'contents', 'preface', 'first', 'sight', 'open', 'book', 'phenomenon', 'invitations', 'blood', 'type', 'scary', 'stories', 'nightmare', 'port', 'angeles', 'theory', 'interrogations', 'complications', 'balancing', 'confessions', 'mind', 'matter', 'cullens', 'carlisle', 'game', 'hunt', 'goodbyes', 'impatience', 'phone', 'call', 'hide-and-seek', 'angel', 'impasse', 'epilogue', 'occasion', 'text', 'copyright', 'stephenie', 'meyer', 'rights', 'reserved', 'little', 'brown', 'company', 'time', 'warner', 'book', 'group', 'avenue', 'americas', 'new', 'york', 'ny', 'visit', 'web', 'site', 'www.lb-teens.com', 'first', 'edition', 'september', 'characters', 'events', 'portrayed', 'book', 'fictitious', 'similarity', 'real', 'persons', 'living', 'dead', 'coincidental', 'intended', 'author', 'library', 'congress', 'cataloging-in-publication', 'data', 'meyer', 'stephanie', 'twilight', 'novel', 'stephanie', 'meyer', '1st', 'ed', 'summary', 'grade', 'up®cheadstrong', 'sun-loving', '17-year-old', 'bella', 'declines', 'mom', \"'s\", 'invitation', 'move', 'florida']\n"
     ]
    }
   ],
   "source": [
    "#Processing of the two documents\n",
    "#0.import the necessary packages\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#For fifty shades of grey\n",
    "#1. Tokenize fiftytext\n",
    "fiftytokens = nltk.word_tokenize(fiftytext) \n",
    "twilighttokens = nltk.word_tokenize(twilighttext) \n",
    "\n",
    "#2. Use all lower cases\n",
    "fiftywords = [w.lower( ) for w in fiftytokens] \n",
    "\n",
    "#3. eliminate all the punctuations from those lowercased-tokens cuz they annoying\n",
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if (pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "alphafiftywords = [w for w in fiftywords if not alpha_filter(w)]\n",
    "\n",
    "#print(alphafiftywords[:100])\n",
    "\n",
    "#4. eliminate the stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stoppedfiftywords = [w for w in alphafiftywords if not w in stopwords]\n",
    "\n",
    "\n",
    "#Run these steps again for the processiong of Twilight\n",
    "#1. Tokenize twilight text\n",
    "twilighttokens = nltk.word_tokenize(twilighttext) \n",
    "\n",
    "#2. Use all lower cases\n",
    "twilightwords = [w.lower( ) for w in twilighttokens] \n",
    "\n",
    "#3. eliminate all the punctuations from those lowercased-tokens cuz they annoying\n",
    "alphatwilightwords = [w for w in twilightwords if not alpha_filter(w)]\n",
    "\n",
    "#print(alphafiftywords[:100])\n",
    "\n",
    "#4. eliminate the stopwords\n",
    "stoppedtwilightwords = [w for w in alphatwilightwords if not w in stopwords]\n",
    "\n",
    "\n",
    "#I think I've done processing of both of them? Let me check it\n",
    "\n",
    "print('stoppedfiftywords:', stoppedfiftywords[0:100],\"\\n\\nstoppedtwilightwords:\", stoppedtwilightwords[0:100])\n",
    "\n",
    "#Looking good!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 words by normalized frequency of Fifty Shades:\n",
      "\n",
      "'s \t 0.013553417170683693\n",
      "n't \t 0.007563493194151244\n",
      "christian \t 0.005523681245386139\n",
      "'m \t 0.005038011733775401\n",
      "eyes \t 0.0034579669226684627\n",
      "like \t 0.003185991996166449\n",
      "want \t 0.003030577752451012\n",
      "grey \t 0.002959346224081437\n",
      "back \t 0.002609664175721705\n",
      "know \t 0.002577286208280989\n",
      "anastasia \t 0.0025190058668877005\n",
      "oh \t 0.002512530273399557\n",
      "kate \t 0.0022276041599212566\n",
      "'re \t 0.0021175190706228227\n",
      "head \t 0.0021175190706228227\n",
      "'ll \t 0.002065714322717677\n",
      "think \t 0.002039811948765104\n",
      "one \t 0.0019815316073718155\n",
      "'ve \t 0.0018390685506326655\n",
      "steele \t 0.0018325929571445224\n",
      "hand \t 0.0017743126157512335\n",
      "feel \t 0.0016253739655239404\n",
      "going \t 0.0015929959980832244\n",
      "says \t 0.001586520404595081\n",
      "hands \t 0.0015541424371543651\n",
      "see \t 0.0015347156566899356\n",
      "time \t 0.0014958620957610765\n",
      "miss \t 0.0014958620957610765\n",
      "ca \t 0.001405203786927072\n",
      "hair \t 0.0013469234455337831\n",
      "ana \t 0.00134044785204564\n",
      "voice \t 0.00134044785204564\n",
      "would \t 0.00134044785204564\n",
      "take \t 0.001308069884604924\n",
      "look \t 0.0012951186976286376\n",
      "good \t 0.0012951186976286376\n",
      "yes \t 0.0012951186976286376\n",
      "mouth \t 0.0012821675106523512\n",
      "go \t 0.001275691917164208\n",
      "need \t 0.001269216323676065\n",
      "well \t 0.0012627407301879217\n",
      "way \t 0.001236838356235349\n",
      "face \t 0.001165606827865774\n",
      "get \t 0.0011397044539132011\n",
      "still \t 0.0011267532669369147\n",
      "'d \t 0.0011138020799606283\n",
      "smile \t 0.0011138020799606283\n",
      "around \t 0.0010749485190317692\n",
      "bed \t 0.0010360949581029102\n",
      "may \t 0.0010360949581029102\n",
      "\n",
      "top 50 words by normalized frequency of Fifty Shades:\n",
      "\n",
      "n't \t 0.01180594435383754\n",
      "'s \t 0.007781375007099795\n",
      "could \t 0.005111852194445121\n",
      "eyes \t 0.004024569346737746\n",
      "edward \t 0.0035458403317024092\n",
      "would \t 0.0034646998206794705\n",
      "said \t 0.0031320237254854233\n",
      "'d \t 0.0030589972655647784\n",
      "back \t 0.0030021989078487217\n",
      "like \t 0.0029535146012349585\n",
      "voice \t 0.0027425492725753186\n",
      "asked \t 0.002677636863756968\n",
      "face \t 0.002515355841711091\n",
      "'m \t 0.0024342153306881526\n",
      "one \t 0.0023368467174606266\n",
      "see \t 0.0022881624108468633\n",
      "looked \t 0.002158337593210162\n",
      "still \t 0.0020853111332895173\n",
      "know \t 0.0020285127755734606\n",
      "away \t 0.001898687957936759\n",
      "'re \t 0.0018337755491184082\n",
      "alice \t 0.0017607490891977638\n",
      "'ll \t 0.00175263503809547\n",
      "charlie \t 0.0017120647825840008\n",
      "time \t 0.001695836680379413\n",
      "bella \t 0.0016877226292771191\n",
      "going \t 0.0016877226292771191\n",
      "around \t 0.0016877226292771191\n",
      "think \t 0.0016877226292771191\n",
      "get \t 0.0015741259138450053\n",
      "head \t 0.001444301096208304\n",
      "hand \t 0.001444301096208304\n",
      "way \t 0.001395616789594541\n",
      "go \t 0.001395616789594541\n",
      "much \t 0.0013793886873899532\n",
      "door \t 0.0013469324829807779\n",
      "room \t 0.0013225903296738963\n",
      "us \t 0.0013225903296738963\n",
      "look \t 0.0012982481763670149\n",
      "something \t 0.001282020074162427\n",
      "right \t 0.001233335767548664\n",
      "well \t 0.0012252217164463702\n",
      "thought \t 0.0012089936142417824\n",
      "never \t 0.0011521952565257255\n",
      "sure \t 0.0011521952565257255\n",
      "want \t 0.0011440812054234317\n",
      "seemed \t 0.001135967154321138\n",
      "even \t 0.0011278531032188441\n",
      "though \t 0.0011116250010142564\n",
      "first \t 0.001087282847707375\n"
     ]
    }
   ],
   "source": [
    "#1. list the top 50 words by frequency (normalized by the length of the document)\n",
    "#For fifty shades\n",
    "fiftydist = nltk.FreqDist(stoppedfiftywords)\n",
    "fiftyitems = fiftydist.most_common(50)\n",
    "print (\"top 50 words by normalized frequency of Fifty Shades:\\n\")\n",
    "for item in fiftyitems:\n",
    "    print (item[0], '\\t',item[1]/len(alphafiftywords))\n",
    "\n",
    "#For Twilight\n",
    "twilightdist = nltk.FreqDist(stoppedtwilightwords)\n",
    "twilightitems = twilightdist.most_common(50)\n",
    "print (\"\\ntop 50 words by normalized frequency of Fifty Shades:\\n\")\n",
    "for item in twilightitems:\n",
    "    print (item[0], '\\t',item[1]/len(alphatwilightwords))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't like some of the words in the word frequency list because 1) some of them do not seem to contribute to the analysis of the books' subjects and styles (could, would), and some of them look like they are not cleaned up thoroughly('s,n't,'ll), so I manually added them to the stopword list and run the word frequency again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Manually add some stopwords to the stopwords list\n",
    "stopwords.extend((\"'s\",\"n't\",\"'m\",\"'ll\",\"'d\",\"'ll\",\"'re\",\"'ve\",\"could\",\"would\"))\n",
    "\n",
    "#4. eliminate the stopwords\n",
    "stoppedfiftywords = [w for w in alphafiftywords if not w in stopwords]\n",
    "stoppedtwilightwords = [w for w in alphatwilightwords if not w in stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New top 50 words by normalized frequency of Fifty Shades:\n",
      "\n",
      "christian \t 0.005523681245386139\n",
      "eyes \t 0.0034579669226684627\n",
      "like \t 0.003185991996166449\n",
      "want \t 0.003030577752451012\n",
      "grey \t 0.002959346224081437\n",
      "back \t 0.002609664175721705\n",
      "know \t 0.002577286208280989\n",
      "anastasia \t 0.0025190058668877005\n",
      "oh \t 0.002512530273399557\n",
      "kate \t 0.0022276041599212566\n",
      "head \t 0.0021175190706228227\n",
      "think \t 0.002039811948765104\n",
      "one \t 0.0019815316073718155\n",
      "steele \t 0.0018325929571445224\n",
      "hand \t 0.0017743126157512335\n",
      "feel \t 0.0016253739655239404\n",
      "going \t 0.0015929959980832244\n",
      "says \t 0.001586520404595081\n",
      "hands \t 0.0015541424371543651\n",
      "see \t 0.0015347156566899356\n",
      "time \t 0.0014958620957610765\n",
      "miss \t 0.0014958620957610765\n",
      "ca \t 0.001405203786927072\n",
      "hair \t 0.0013469234455337831\n",
      "ana \t 0.00134044785204564\n",
      "voice \t 0.00134044785204564\n",
      "take \t 0.001308069884604924\n",
      "look \t 0.0012951186976286376\n",
      "good \t 0.0012951186976286376\n",
      "yes \t 0.0012951186976286376\n",
      "mouth \t 0.0012821675106523512\n",
      "go \t 0.001275691917164208\n",
      "need \t 0.001269216323676065\n",
      "well \t 0.0012627407301879217\n",
      "way \t 0.001236838356235349\n",
      "face \t 0.001165606827865774\n",
      "get \t 0.0011397044539132011\n",
      "still \t 0.0011267532669369147\n",
      "smile \t 0.0011138020799606283\n",
      "around \t 0.0010749485190317692\n",
      "bed \t 0.0010360949581029102\n",
      "may \t 0.0010360949581029102\n",
      "really \t 0.001029619364614767\n",
      "holy \t 0.0010166681776384806\n",
      "thought \t 0.0010101925841503373\n",
      "room \t 0.000997241397174051\n",
      "say \t 0.0009713390232214782\n",
      "date \t 0.0009454366492689055\n",
      "looks \t 0.0009195342753163327\n",
      "asks \t 0.0009065830883400464\n",
      "\n",
      "New top 50 words by normalized frequency of Fifty Shades:\n",
      "\n",
      "eyes \t 0.004024569346737746\n",
      "edward \t 0.0035458403317024092\n",
      "said \t 0.0031320237254854233\n",
      "back \t 0.0030021989078487217\n",
      "like \t 0.0029535146012349585\n",
      "voice \t 0.0027425492725753186\n",
      "asked \t 0.002677636863756968\n",
      "face \t 0.002515355841711091\n",
      "one \t 0.0023368467174606266\n",
      "see \t 0.0022881624108468633\n",
      "looked \t 0.002158337593210162\n",
      "still \t 0.0020853111332895173\n",
      "know \t 0.0020285127755734606\n",
      "away \t 0.001898687957936759\n",
      "alice \t 0.0017607490891977638\n",
      "charlie \t 0.0017120647825840008\n",
      "time \t 0.001695836680379413\n",
      "bella \t 0.0016877226292771191\n",
      "going \t 0.0016877226292771191\n",
      "around \t 0.0016877226292771191\n",
      "think \t 0.0016877226292771191\n",
      "get \t 0.0015741259138450053\n",
      "head \t 0.001444301096208304\n",
      "hand \t 0.001444301096208304\n",
      "way \t 0.001395616789594541\n",
      "go \t 0.001395616789594541\n",
      "much \t 0.0013793886873899532\n",
      "door \t 0.0013469324829807779\n",
      "room \t 0.0013225903296738963\n",
      "us \t 0.0013225903296738963\n",
      "look \t 0.0012982481763670149\n",
      "something \t 0.001282020074162427\n",
      "right \t 0.001233335767548664\n",
      "well \t 0.0012252217164463702\n",
      "thought \t 0.0012089936142417824\n",
      "never \t 0.0011521952565257255\n",
      "sure \t 0.0011521952565257255\n",
      "want \t 0.0011440812054234317\n",
      "seemed \t 0.001135967154321138\n",
      "even \t 0.0011278531032188441\n",
      "though \t 0.0011116250010142564\n",
      "first \t 0.001087282847707375\n",
      "tried \t 0.001087282847707375\n",
      "carlisle \t 0.0010710547455027872\n",
      "really \t 0.0010629406944004933\n",
      "mike \t 0.0010629406944004933\n",
      "smiled \t 0.0010467125921959056\n",
      "turned \t 0.0010467125921959056\n",
      "made \t 0.0010385985410936117\n",
      "long \t 0.001030484489991318\n"
     ]
    }
   ],
   "source": [
    "#Run the word frequency a second time\n",
    "#1. list the top 50 words by frequency (normalized by the length of the document)\n",
    "#For fifty shades\n",
    "fiftydist = nltk.FreqDist(stoppedfiftywords)\n",
    "fiftyitems = fiftydist.most_common(50)\n",
    "print (\"New top 50 words by normalized frequency of Fifty Shades:\\n\")\n",
    "for item in fiftyitems:\n",
    "    print (item[0], '\\t',item[1]/len(alphafiftywords))\n",
    "\n",
    "#For Twilight\n",
    "twilightdist = nltk.FreqDist(stoppedtwilightwords)\n",
    "twilightitems = twilightdist.most_common(50)\n",
    "print (\"\\nNew top 50 words by normalized frequency of Fifty Shades:\\n\")\n",
    "for item in twilightitems:\n",
    "    print (item[0], '\\t',item[1]/len(alphatwilightwords))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 bigrams by frequencies of Fifty Shades:\n",
      "(('christian', 'grey'), 0.002897505926716668)\n",
      "(('anastasia', 'steele'), 0.001954776725679666)\n",
      "(('miss', 'steele'), 0.0016359124370936213)\n",
      "(('date', 'may'), 0.0013725028073921061)\n",
      "(('enterprises', 'holdings'), 0.0009843201952003994)\n",
      "(('grey', 'enterprises'), 0.0009843201952003994)\n",
      "(('mr.', 'grey'), 0.0009704565304792669)\n",
      "(('gray', 'eyes'), 0.0009427292010370022)\n",
      "(('grey', 'subject'), 0.0009150018715947374)\n",
      "(('christian', 'greyceo'), 0.0008872745421524726)\n",
      "(('greyceo', 'grey'), 0.0008456835479890754)\n",
      "(('steele', 'subject'), 0.0008179562185468106)\n",
      "(('inner', 'goddess'), 0.0008040925538256783)\n",
      "(('holdings', 'inc.'), 0.0007347742302200163)\n",
      "(('may', 'anastasia'), 0.0007347742302200163)\n",
      "(('miss', 'steele.'), 0.0005961375830086925)\n",
      "(('holy', 'shit'), 0.0005545465888452954)\n",
      "(('shakes', 'head'), 0.0004990919299607658)\n",
      "(('may', 'christian'), 0.0004575009357973687)\n",
      "(('shake', 'head'), 0.0004436372710762363)\n",
      "(('first', 'time'), 0.0004297736063551039)\n",
      "(('holy', 'crap'), 0.0004297736063551039)\n",
      "(('one', 'side'), 0.0004297736063551039)\n",
      "(('voice', 'soft'), 0.0004297736063551039)\n",
      "(('roll', 'eyes'), 0.00041590994163397154)\n",
      "(('want', 'know'), 0.00041590994163397154)\n",
      "(('close', 'eyes'), 0.00040204627691283915)\n",
      "(('inc.', 'anastasia'), 0.00040204627691283915)\n",
      "(('closes', 'eyes'), 0.00038818261219170677)\n",
      "(('date', 'june'), 0.0003743189474705744)\n",
      "(('let', 'go'), 0.0003743189474705744)\n",
      "(('est', 'christian'), 0.000360455282749442)\n",
      "(('head', 'back'), 0.0003465916180283096)\n",
      "(('head', 'one'), 0.0003465916180283096)\n",
      "(('deep', 'breath'), 0.00033272795330717723)\n",
      "(('feel', 'like'), 0.00033272795330717723)\n",
      "(('dominant', 'shall'), 0.00031886428858604485)\n",
      "(('holds', 'hand'), 0.00031886428858604485)\n",
      "(('oh', 'no…'), 0.00031886428858604485)\n",
      "(('katherine', 'kavanagh'), 0.00030500062386491246)\n",
      "(('last', 'night'), 0.00030500062386491246)\n",
      "(('mrs.', 'robinson'), 0.00030500062386491246)\n",
      "(('raises', 'eyebrows'), 0.00030500062386491246)\n",
      "(('mr', 'grey'), 0.0002911369591437801)\n",
      "(('want', 'go'), 0.0002911369591437801)\n",
      "(('oh', 'my…'), 0.0002772732944226477)\n",
      "(('cocks', 'head'), 0.0002634096297015153)\n",
      "(('hand', 'leads'), 0.0002495459649803829)\n",
      "(('june', 'est'), 0.0002495459649803829)\n",
      "(('make', 'way'), 0.0002495459649803829)\n",
      "\n",
      "\n",
      "Top 50 bigrams by frequencies of Twilight:\n",
      "(('shook', 'head'), 0.0007518124049046809)\n",
      "(('looked', 'away'), 0.0005728094513559474)\n",
      "(('edward', 'cullen'), 0.0005370088606462006)\n",
      "(('let', 'go'), 0.00046540767922670725)\n",
      "(('edward', 'said'), 0.0004475073838718339)\n",
      "(('looked', 'like'), 0.0004296070885169605)\n",
      "(('mr', 'banner'), 0.0003938064978072138)\n",
      "(('one', 'hand'), 0.0003580059070974671)\n",
      "(('first', 'time'), 0.00034010561174259375)\n",
      "(('parking', 'lot'), 0.00034010561174259375)\n",
      "(('dr.', 'cullen'), 0.00030430502103284706)\n",
      "(('port', 'angeles'), 0.00030430502103284706)\n",
      "(('sounded', 'like'), 0.00030430502103284706)\n",
      "(('around', 'corner'), 0.0002864047256779737)\n",
      "(('closed', 'eyes'), 0.0002864047256779737)\n",
      "(('alice', 'jasper'), 0.0002685044303231003)\n",
      "(('rolled', 'eyes'), 0.0002685044303231003)\n",
      "(('looking', 'away'), 0.000250604134968227)\n",
      "(('walked', 'away'), 0.000250604134968227)\n",
      "(('want', 'know'), 0.000250604134968227)\n",
      "(('eyes', 'narrowed'), 0.00023270383961335363)\n",
      "(('kept', 'eyes'), 0.00023270383961335363)\n",
      "(('never', 'seen'), 0.00023270383961335363)\n",
      "(('see', 'face'), 0.00023270383961335363)\n",
      "(('eyes', 'still'), 0.00021480354425848025)\n",
      "(('last', 'night'), 0.00021480354425848025)\n",
      "(('long', 'time'), 0.00021480354425848025)\n",
      "(('make', 'sure'), 0.00021480354425848025)\n",
      "(('opened', 'eyes'), 0.00021480354425848025)\n",
      "(('voice', 'sounded'), 0.00021480354425848025)\n",
      "(('alice', 'said'), 0.0001969032489036069)\n",
      "(('anything', 'else'), 0.0001969032489036069)\n",
      "(('bella', 'said'), 0.0001969032489036069)\n",
      "(('door', 'open'), 0.0001969032489036069)\n",
      "(('even', 'though'), 0.0001969032489036069)\n",
      "(('seemed', 'like'), 0.0001969032489036069)\n",
      "(('asked', 'voice'), 0.00017900295354873356)\n",
      "(('behind', 'us'), 0.00017900295354873356)\n",
      "(('bit', 'lip'), 0.00017900295354873356)\n",
      "(('eyes', 'closed'), 0.00017900295354873356)\n",
      "(('far', 'away'), 0.00017900295354873356)\n",
      "(('first', 'day'), 0.00017900295354873356)\n",
      "(('going', 'tell'), 0.00017900295354873356)\n",
      "(('looked', 'see'), 0.00017900295354873356)\n",
      "(('opened', 'door'), 0.00017900295354873356)\n",
      "(('said', 'voice'), 0.00017900295354873356)\n",
      "(('say', 'anything'), 0.00017900295354873356)\n",
      "(('voice', 'still'), 0.00017900295354873356)\n",
      "(('across', 'face'), 0.0001611026581938602)\n",
      "(('alice', 'asked'), 0.0001611026581938602)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2. list the top 50 bigrams by frequencies\n",
    "#For fifty shades\n",
    "# setup for bigrams and bigram measures\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder = BigramCollocationFinder.from_words(stoppedfiftywords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "#scores are bigram frequencies normalized by dividing by the total number of bigrams\n",
    "#scores are sorted in decreasing frequency\n",
    "print (\"Top 50 bigrams by frequencies of Fifty Shades:\")\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)\n",
    "print(\"\\n\")\n",
    "\n",
    "#For Twilight\n",
    "finder = BigramCollocationFinder.from_words(stoppedtwilightwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "#scores are bigram frequencies normalized by dividing by the total number of bigrams\n",
    "#scores are sorted in decreasing frequency\n",
    "print (\"Top 50 bigrams by frequencies of Twilight:\")\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 bigrams by mutual information scores of Fifty Shades:\n",
      "(('boxer', 'briefs'), 13.816403709653716)\n",
      "(('steering', 'wheel'), 13.55336930381992)\n",
      "(('anal', 'intercourse'), 12.845550055313232)\n",
      "(('scalp', 'prickles'), 12.678900185903776)\n",
      "(('orange', 'juice'), 12.526897092458727)\n",
      "(('heaven', 'sake'), 12.24524700845759)\n",
      "(('safety', 'procedures'), 12.231441208932559)\n",
      "(('charlie', 'tango'), 12.138331804541076)\n",
      "(('acts', 'involving'), 11.89040429109749)\n",
      "(('fifty', 'shades'), 11.816403709653715)\n",
      "(('class', 'lounge'), 11.746014381762318)\n",
      "(('cable', 'ties'), 11.67890018590378)\n",
      "(('brow', 'furrows'), 11.668011869761042)\n",
      "(('foil', 'packet'), 11.500901883925783)\n",
      "(('dr.', 'flynn'), 11.437892086399986)\n",
      "(('finds', 'release'), 11.3727970581781)\n",
      "(('dr.', 'greene'), 11.330976882483473)\n",
      "(('squirm', 'uncomfortably'), 11.138331804541076)\n",
      "(('index', 'finger'), 11.093937685182622)\n",
      "(('personal', 'trainer'), 11.050868963290737)\n",
      "(('parking', 'lot'), 10.968406803098762)\n",
      "(('computer', 'loan'), 10.830903279348831)\n",
      "(('shoes', 'socks'), 10.746014381762318)\n",
      "(('nowhere', 'seen'), 10.602278904300867)\n",
      "(('sexual', 'activity'), 10.602278904300867)\n",
      "(('raises', 'eyebrows'), 10.569857426608486)\n",
      "(('june', 'est'), 10.546705573538906)\n",
      "(('shrug', 'apologetically'), 10.535447395822658)\n",
      "(('riding', 'crop'), 10.47726632473413)\n",
      "(('without', 'hesitation'), 10.475366791818644)\n",
      "(('ear', 'buds'), 10.437892086399986)\n",
      "(('bar', 'stools'), 10.290334897986128)\n",
      "(('private', 'joke'), 10.290334897986128)\n",
      "(('laters', 'baby'), 10.231441208932559)\n",
      "(('sweat', 'pants'), 10.231441208932559)\n",
      "(('english', 'breakfast'), 10.231441208932557)\n",
      "(('husband', 'number'), 10.219468567266482)\n",
      "(('mrs.', 'jones'), 10.217766272035481)\n",
      "(('katherine', 'kavanagh'), 10.18837248704067)\n",
      "(('control', 'freak'), 10.166346180710674)\n",
      "(('fully', 'aware'), 10.16105188104116)\n",
      "(('inner', 'goddess'), 10.15947343991993)\n",
      "(('linen', 'shirt'), 10.121523516854522)\n",
      "(('breathing', 'ragged'), 10.103685661734183)\n",
      "(('chest', 'drawers'), 10.09393768518262)\n",
      "(('allotted', 'times'), 10.083049369039886)\n",
      "(('mrs.', 'robinson'), 10.062488046557567)\n",
      "(('enterprises', 'holdings'), 9.988584685036395)\n",
      "(('holdings', 'inc.'), 9.988584685036393)\n",
      "(('holdings', 'inc'), 9.988584685036392)\n",
      "\n",
      "\n",
      "Top 50 bigrams by mutual information scores of Twilight:\n",
      "(('1st', 'ed'), 15.769657082282718)\n",
      "(('40x', 'objective'), 15.769657082282718)\n",
      "(('abstinence', 'resented'), 15.769657082282718)\n",
      "(('acquaintances', 'considerately'), 15.769657082282718)\n",
      "(('acquaintances°≠', 'homesick'), 15.769657082282718)\n",
      "(('actors', 'portray'), 15.769657082282718)\n",
      "(('addicted', 'illegal'), 15.769657082282718)\n",
      "(('adjoining', 'handkerchief-sized'), 15.769657082282718)\n",
      "(('admired', 'civility'), 15.769657082282718)\n",
      "(('advance', 'slinking'), 15.769657082282718)\n",
      "(('advanced', 'placement'), 15.769657082282718)\n",
      "(('affidavits', 'well-known'), 15.769657082282718)\n",
      "(('afterward', 'ill'), 15.769657082282718)\n",
      "(('agent', 'jodi'), 15.769657082282718)\n",
      "(('agonizing', 'outcome'), 15.769657082282718)\n",
      "(('alphabetized', 'listing'), 15.769657082282718)\n",
      "(('american', 'pastime'), 15.769657082282718)\n",
      "(('anemones', 'undulated'), 15.769657082282718)\n",
      "(('angelfish', 'shark'), 15.769657082282718)\n",
      "(('anger°≠', 'pain°≠'), 15.769657082282718)\n",
      "(('appallingly', 'luscious'), 15.769657082282718)\n",
      "(('appatently', 'notion'), 15.769657082282718)\n",
      "(('aquarium', 'bouquets'), 15.769657082282718)\n",
      "(('areas', 'overpopulation'), 15.769657082282718)\n",
      "(('are°≠', 'nice-looking'), 15.769657082282718)\n",
      "(('aro', 'marcus'), 15.769657082282718)\n",
      "(('array', 'sodas'), 15.769657082282718)\n",
      "(('askance', 'luggage-less'), 15.769657082282718)\n",
      "(('asset', 'community'), 15.769657082282718)\n",
      "(('austen', 'selected'), 15.769657082282718)\n",
      "(('austere', 'soaring'), 15.769657082282718)\n",
      "(('avenue', 'americas'), 15.769657082282718)\n",
      "(('aversion', \"'his\"), 15.769657082282718)\n",
      "(('awakened', 'renewed'), 15.769657082282718)\n",
      "(('awards', 'cluttering'), 15.769657082282718)\n",
      "(('babysitters', 'handled'), 15.769657082282718)\n",
      "(('bacon', 'requested'), 15.769657082282718)\n",
      "(('balloon', 'arches'), 15.769657082282718)\n",
      "(('battery', 'cables'), 15.769657082282718)\n",
      "(('battle', 'blizzard'), 15.769657082282718)\n",
      "(('bell-like', 'echoes'), 15.769657082282718)\n",
      "(('billed', 'semiformal'), 15.769657082282718)\n",
      "(('binding', 'motif'), 15.769657082282718)\n",
      "(('biohazard', 'suit'), 15.769657082282718)\n",
      "(('bleached', 'bone'), 15.769657082282718)\n",
      "(('blend', 'succeed'), 15.769657082282718)\n",
      "(('blond°≠', 'handsomer'), 15.769657082282718)\n",
      "(('bond', 'convenience'), 15.769657082282718)\n",
      "(('boundless', 'labyrinth'), 15.769657082282718)\n",
      "(('bowls', 'valleys'), 15.769657082282718)\n"
     ]
    }
   ],
   "source": [
    "#3. list the top 50 bigrams by their Mutual Information scores (using min frequency 5)\n",
    "#For fifty shades\n",
    "finder2 = BigramCollocationFinder.from_words(stoppedfiftywords)\n",
    "finder2.apply_freq_filter(5)\n",
    "scored = finder2.score_ngrams(bigram_measures.pmi)\n",
    "print (\"Top 50 bigrams by mutual information scores of Fifty Shades:\")\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)\n",
    "print('\\n')\n",
    "\n",
    "        \n",
    "#For Twilight\n",
    "finder2 = BigramCollocationFinder.from_words(stoppedtwilightwords)\n",
    "scored = finder2.score_ngrams(bigram_measures.pmi)\n",
    "print (\"Top 50 bigrams by mutual information scores of Twilight:\")\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 trigrams by frequencies of Fifty Shades:\n",
      "(('grey', 'enterprises', 'holdings'), 0.0009843201952003994)\n",
      "(('christian', 'grey', 'subject'), 0.0009150018715947374)\n",
      "(('christian', 'greyceo', 'grey'), 0.0008456835479890754)\n",
      "(('greyceo', 'grey', 'enterprises'), 0.0008456835479890754)\n",
      "(('anastasia', 'steele', 'subject'), 0.0008179562185468106)\n",
      "(('enterprises', 'holdings', 'inc.'), 0.0007347742302200163)\n",
      "(('date', 'may', 'anastasia'), 0.000720910565498884)\n",
      "(('may', 'anastasia', 'steele'), 0.000720910565498884)\n",
      "(('date', 'may', 'christian'), 0.0004436372710762363)\n",
      "(('may', 'christian', 'grey'), 0.0004297736063551039)\n",
      "(('holdings', 'inc.', 'anastasia'), 0.00040204627691283915)\n",
      "(('inc.', 'anastasia', 'steele'), 0.00040204627691283915)\n",
      "(('est', 'christian', 'grey'), 0.00033272795330717723)\n",
      "(('head', 'one', 'side'), 0.00031886428858604485)\n",
      "(('date', 'june', 'est'), 0.0002495459649803829)\n",
      "(('anastasia', 'steele', 'dear'), 0.00023568230025925054)\n",
      "(('cocks', 'head', 'one'), 0.00023568230025925054)\n",
      "(('christian', 'grey', 'dear'), 0.00022181863553811815)\n",
      "(('enterprises', 'holdings', 'inc'), 0.00022181863553811815)\n",
      "(('mouth', 'drops', 'open'), 0.00020795497081698577)\n",
      "(('steele', 'dear', 'miss'), 0.00020795497081698577)\n",
      "(('miss', 'steele', 'says'), 0.00019409130609585339)\n",
      "(('date', 'may', 'est'), 0.000180227641374721)\n",
      "(('june', 'est', 'christian'), 0.000180227641374721)\n",
      "(('may', 'est', 'christian'), 0.000180227641374721)\n",
      "(('runs', 'hand', 'hair'), 0.00016636397665358862)\n",
      "(('miss', 'steele', 'murmurs'), 0.00015250031193245623)\n",
      "(('grey', 'dear', 'mr.'), 0.00013863664721132385)\n",
      "(('date', 'june', 'anastasia'), 0.00012477298249019146)\n",
      "(('dear', 'miss', 'steelei'), 0.00012477298249019146)\n",
      "(('red', 'room', 'pain'), 0.00012477298249019146)\n",
      "(('ana', 'christian', 'grey'), 0.00011090931776905908)\n",
      "(('anastasia', 'steele', 'miss'), 0.00011090931776905908)\n",
      "(('june', 'anastasia', 'steele'), 0.00011090931776905908)\n",
      "(('mouth', 'presses', 'hard'), 0.00011090931776905908)\n",
      "(('presses', 'hard', 'line'), 0.00011090931776905908)\n",
      "(('takes', 'hand', 'leads'), 0.00011090931776905908)\n",
      "(('taking', 'deep', 'breath'), 0.00011090931776905908)\n",
      "(('white', 'linen', 'shirt'), 0.00011090931776905908)\n",
      "(('long', 'index', 'finger'), 9.704565304792669e-05)\n",
      "(('put', 'head', 'hands'), 9.704565304792669e-05)\n",
      "(('takes', 'deep', 'breath'), 9.704565304792669e-05)\n",
      "(('taking', 'hand', 'leads'), 9.704565304792669e-05)\n",
      "(('gray', 'eyes', 'blaze'), 8.318198832679431e-05)\n",
      "(('hair', 'behind', 'ear'), 8.318198832679431e-05)\n",
      "(('hands', 'either', 'side'), 8.318198832679431e-05)\n",
      "(('leans', 'forward', 'kisses'), 8.318198832679431e-05)\n",
      "(('really', 'want', 'know'), 8.318198832679431e-05)\n",
      "(('reply', 'anastasia', 'steele'), 8.318198832679431e-05)\n",
      "(('submissive', 'personal', 'trainer'), 8.318198832679431e-05)\n",
      "\n",
      "\n",
      "Top 50 trigrams by frequencies of Twilight:\n",
      "(('took', 'deep', 'breath'), 0.00014320236283898684)\n",
      "(('raised', 'one', 'eyebrow'), 0.00010740177212924013)\n",
      "(('arm', 'around', 'waist'), 7.160118141949342e-05)\n",
      "(('face', 'inches', 'mine'), 7.160118141949342e-05)\n",
      "(('put', 'seat', 'belt'), 7.160118141949342e-05)\n",
      "(('answer', 'yes', 'yes'), 5.3700886064620063e-05)\n",
      "(('back', 'walked', 'away'), 5.3700886064620063e-05)\n",
      "(('bella', 'edward', 'voice'), 5.3700886064620063e-05)\n",
      "(('breathed', 'sigh', 'relief'), 5.3700886064620063e-05)\n",
      "(('engine', 'roared', 'life'), 5.3700886064620063e-05)\n",
      "(('going', 'seattle', 'day'), 5.3700886064620063e-05)\n",
      "(('held', 'phone', 'ear'), 5.3700886064620063e-05)\n",
      "(('kept', 'eyes', 'away'), 5.3700886064620063e-05)\n",
      "(('last', 'time', 'seen'), 5.3700886064620063e-05)\n",
      "(('left', 'jacket', 'car'), 5.3700886064620063e-05)\n",
      "(('light', 'brown', 'hair'), 5.3700886064620063e-05)\n",
      "(('like', 'everyone', 'else'), 5.3700886064620063e-05)\n",
      "(('like', 'long', 'time'), 5.3700886064620063e-05)\n",
      "(('lips', 'pressed', 'together'), 5.3700886064620063e-05)\n",
      "(('mr', 'banner', 'asked'), 5.3700886064620063e-05)\n",
      "(('pressed', 'lips', 'together'), 5.3700886064620063e-05)\n",
      "(('shut', 'door', 'behind'), 5.3700886064620063e-05)\n",
      "(('squeezed', 'eyes', 'shut'), 5.3700886064620063e-05)\n",
      "(('took', 'face', 'hands'), 5.3700886064620063e-05)\n",
      "(('took', 'step', 'back'), 5.3700886064620063e-05)\n",
      "(('tried', 'keep', 'voice'), 5.3700886064620063e-05)\n",
      "(('turned', 'walked', 'away'), 5.3700886064620063e-05)\n",
      "(('able', 'know', 'thinking'), 3.580059070974671e-05)\n",
      "(('alice', 'carlisle', 'asked'), 3.580059070974671e-05)\n",
      "(('almost', 'ninety', 'years'), 3.580059070974671e-05)\n",
      "(('another', 'bite', 'pizza'), 3.580059070974671e-05)\n",
      "(('anything', 'monday', 'night'), 3.580059070974671e-05)\n",
      "(('anything', 'wrong', 'bella'), 3.580059070974671e-05)\n",
      "(('arms', 'across', 'chest'), 3.580059070974671e-05)\n",
      "(('arms', 'tightly', 'across'), 3.580059070974671e-05)\n",
      "(('asked', 'curiously', 'looked'), 3.580059070974671e-05)\n",
      "(('asked', 'looked', 'see'), 3.580059070974671e-05)\n",
      "(('asked', 'nothing', 'wrong'), 3.580059070974671e-05)\n",
      "(('asked', 'one', 'answered'), 3.580059070974671e-05)\n",
      "(('asked', 'raising', 'eyebrows'), 3.580059070974671e-05)\n",
      "(('asked', 'turning', 'back'), 3.580059070974671e-05)\n",
      "(('away', 'eyes', 'wandering'), 3.580059070974671e-05)\n",
      "(('back', 'corner', 'truck'), 3.580059070974671e-05)\n",
      "(('back', 'look', 'face'), 3.580059070974671e-05)\n",
      "(('back', 'soon', 'promised'), 3.580059070974671e-05)\n",
      "(('back', 'time', 'dance'), 3.580059070974671e-05)\n",
      "(('believe', 'give', 'easily'), 3.580059070974671e-05)\n",
      "(('bell', 'rang', 'last'), 3.580059070974671e-05)\n",
      "(('bella', 'alice', 'said'), 3.580059070974671e-05)\n",
      "(('bella', 'mike', 'said'), 3.580059070974671e-05)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run trigrams just for fun\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder3 = TrigramCollocationFinder.from_words(stoppedfiftywords)\n",
    "scored = finder3.score_ngrams(trigram_measures.raw_freq)\n",
    "print (\"Top 50 trigrams by frequencies of Fifty Shades:\")\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)\n",
    "print('\\n')\n",
    "\n",
    "finder3 = TrigramCollocationFinder.from_words(stoppedtwilightwords)\n",
    "scored = finder3.score_ngrams(trigram_measures.raw_freq)\n",
    "print (\"Top 50 trigrams by frequencies of Twilight:\")\n",
    "for bscore in scored[:50]:\n",
    "    print (bscore)\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Describe a problem or question that is based on the difference between the two documents**\n",
    "\n",
    "**Again, my question is: What do the themes of Fifty Shades and Twilight have in common and what do they differ?**\n",
    "\n",
    "\n",
    "From the bigram frequency distributions, I can see that both books might be narrated from a female's perspective, as they both described the love interest of the female protagonist with quite a bit of words, which coincides with these books' core audiences: female.\n",
    " \n",
    "In \"Top 50 bigrams by frequencies of Fifty Shades\", bigrams  ('christian', 'grey') is the no.1 on the list, as well as ('gray', 'eyes'), ('grey', 'subject'),('grey', 'subject'), which are all the instances of describing Christian Grey, the protagonist Anastasia Steele's love interest. Not suprisingly, \"christian\"'s frequency is more than twice as high as \"anastasia\" on the word frequency list of Fifty Shades.\n",
    "\n",
    "Similarly, in \"Top 50 bigrams by frequencies of Twilight\", ('edward', 'cullen') and ('edward', 'said') were no.3 and no.5 on the list, and \"edward\" appeared to be mentioned as twice as may as \"bella\" on the word frequency list. Again, this Edward Cullen guy was the love interest of the female protagonist Bella Swan. \n",
    "\n",
    "There are also some similarities in terms of describing a person in two books, as the word \"eyes\" appeared to be second and first on both books' word frequency list, respectively. They might be used to describe Christian and Edward.\n",
    "\n",
    "However, there is some differences in narrating style, as Fifty Shades seemed to be narrated from the first person point of view while Twilight was narrated from the third person POV. As I saw ('bella', 'said') in the Top 50 bigrams by frequencies of Twilight, but I couldn't seem to find ('anastasia', 'said') or something similar from Top 50 bigrams by frequencies of Fifty Shades.\n",
    "\n",
    "One thing further reinforces the previous difference I made is, the characters in both books seem to be shaking their heads a lot, as ('shakes', 'head') and ('shook', 'head') made it on the both Top 50 bigram frequency lists respectively. However as you could see, Fifty shades uses the present tense \"shakes\" while Twilight uses past tense \"shook\", which might be because of the difference of POV in narrating of both books.\n",
    "\n",
    "In addition, it looks like the subject matter of Fifty Shades is more mature oriented, with the appearances of bigrams like ('cocks', 'head'), ('dominant', 'shall') on the bigram raw frequency list, and ('boxer', 'briefs'),(('anal', 'intercourse') on the bigram MIS list, and someone likes to exclaim ('holy', 'shit') a lot in Fifty Shades, whereas similar mature subject matter and language were not to be found in the analysis of Twilight. This comes no surprise, as Anastasia and Christian are 20-something adults, while Bella and Edward were dating in the high school in the book.\n",
    "\n",
    "This is just some preliminary analysis, but it has been a lot of fun running it, and I could not wait to apply more advanced NLP techniques to do some more in-depth analysis in the future.I am pretty sure the process of doing this homework is more fun than actually reading through these two books.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
